import json
import torch
import faiss
import numpy as np
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
from pathlib import Path
import os

# --- é…ç½® ---
MODEL_NAME = "witiko/mathberta"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BATCH_SIZE = 256  # 3090 æ˜¾å­˜å¤§ï¼Œå¯ä»¥è®¾é«˜æå‡åå
CHECKPOINT_STEP = 50000  # æ¯ 5 ä¸‡æ¡å…¬å¼ä¿å­˜ä¸€æ¬¡æ–­ç‚¹
ARTIFACTS_DIR = Path("artifacts")
INDEX_PATH = ARTIFACTS_DIR / "vector_index.faiss"
MAPPING_PATH = ARTIFACTS_DIR / "vector_id_mapping.json"
STATE_PATH = ARTIFACTS_DIR / "build_state.json"

class MathVectorEngine:
    def __init__(self):
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ {MODEL_NAME} åˆ° {DEVICE}...")
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        self.model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)
        self.model.eval()

    def encode(self, latex_list):
        inputs = self.tokenizer(latex_list, padding=True, truncation=True, 
                                 max_length=128, return_tensors="pt").to(DEVICE)
        with torch.no_grad():
            outputs = self.model(**inputs)
            embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
        return embeddings

def load_state():
    """è¯»å–æ–­ç‚¹ä¿¡æ¯"""
    if STATE_PATH.exists():
        with open(STATE_PATH, 'r') as f:
            return json.load(f)
    return {"last_processed_idx": 0}

def save_state(idx):
    """ä¿å­˜æ–­ç‚¹ä¿¡æ¯"""
    with open(STATE_PATH, 'w') as f:
        json.dump({"last_processed_idx": idx}, f)

def build_index():
    ARTIFACTS_DIR.mkdir(exist_ok=True)
    
    # 1. åŠ è½½æ•°æ®
    print("ğŸ“– æ­£åœ¨è¯»å– formulas.json ...")
    with open("data/processed/formulas.json", 'r', encoding='utf-8') as f:
        corpus = json.load(f)
    
    fids = list(corpus.keys())
    # 2. è¿™é‡Œçš„å†…å­˜ç®¡ç†ï¼šåªä¿ç•™å½“å‰éœ€è¦çš„åˆ—è¡¨ï¼Œå°½å¿«é‡Šæ”¾ corpus
    latex_list = [corpus[fid]['latex_norm'] for fid in fids]
    del corpus # é‡Šæ”¾å¤§å­—å…¸ï¼Œè…¾å‡ºå†…å­˜ç»™å‘é‡
    
    # 3. åˆå§‹åŒ–æˆ–æ¢å¤ç´¢å¼•
    state = load_state()
    start_idx = state["last_processed_idx"]
    dimension = 768
    
    if start_idx > 0 and INDEX_PATH.exists():
        print(f"ğŸ”„ æ£€æµ‹åˆ°æ–­ç‚¹ï¼Œå‡†å¤‡ä»ç¬¬ {start_idx:,} æ¡å…¬å¼ç»§ç»­...")
        index = faiss.read_index(str(INDEX_PATH))
        with open(MAPPING_PATH, 'r') as f:
            saved_fids = json.load(f)
    else:
        print("ğŸ—ï¸ åˆå§‹åŒ–å…¨æ–°ç´¢å¼•...")
        # é’ˆå¯¹ 30GB å†…å­˜ï¼Œå¦‚æœå…¨é‡è·‘ 1300 ä¸‡ï¼ŒåæœŸå»ºè®®æ¢æˆ IndexIVFPQ (å‹ç¼©ç´¢å¼•)
        # ç›®å‰ 5 åˆ†ç‰‡æµ‹è¯•ï¼ŒIndexFlatIP å®Œå…¨æ²¡é—®é¢˜
        index = faiss.IndexFlatIP(dimension)
        saved_fids = []
        start_idx = 0

    engine = MathVectorEngine()
    
    # 4. å¾ªç¯ç¼–ç 
    print(f"ğŸš€ å¼€å§‹å‘é‡åŒ– (ç›®æ ‡: {len(latex_list):,} æ¡)...")
    pbar = tqdm(total=len(latex_list), initial=start_idx, desc="å‘é‡ç¼–ç ")
    
    for i in range(start_idx, len(latex_list), BATCH_SIZE):
        end_idx = min(i + BATCH_SIZE, len(latex_list))
        batch = latex_list[i : end_idx]
        batch_fids = fids[i : end_idx]
        
        try:
            emb = engine.encode(batch)
            faiss.normalize_L2(emb)
            index.add(emb)
            saved_fids.extend(batch_fids)
            
            # å®šæœŸä¿å­˜æ–­ç‚¹ï¼Œé˜²æ­¢å´©æºƒ
            if (i + BATCH_SIZE) % CHECKPOINT_STEP == 0 or end_idx == len(latex_list):
                faiss.write_index(index, str(INDEX_PATH))
                with open(MAPPING_PATH, 'w') as f:
                    json.dump(saved_fids, f)
                save_state(end_idx)
            
            pbar.update(len(batch))
        except Exception as e:
            print(f"\nâŒ å‡ºé”™äºç´¢å¼• {i}: {e}")
            continue
            
    pbar.close()
    print(f"âœ… å®Œæˆï¼æ€»ç´¢å¼•æ•°: {index.ntotal:,}")

if __name__ == "__main__":
    build_index()