"""
å®Œæ•´çš„æ•°æ®é¢„å¤„ç†å·¥ä½œæµè‡ªåŠ¨åŒ–è„šæœ¬
ä¸€é”®è§£å†³æ‰€æœ‰å·²çŸ¥é—®é¢˜å¹¶ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
"""

import subprocess
import json
import sys
from pathlib import Path
from datetime import datetime
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('workflow.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============================================================
# ğŸš€ æ ¸å¿ƒ1: æ­¥éª¤æ‰§è¡Œå™¨
# ============================================================
class WorkflowStep:
    def __init__(self, name, script, required_inputs=None, outputs=None):
        self.name = name
        self.script = script
        self.required_inputs = required_inputs or []
        self.outputs = outputs or []
        self.success = False
        self.error_msg = None
    
    def check_prerequisites(self):
        """æ£€æŸ¥å‰ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        missing = [f for f in self.required_inputs if not Path(f).exists()]
        if missing:
            return False, f"Missing files: {missing}"
        return True, None
    
    def execute(self):
        """æ‰§è¡Œæ­¥éª¤"""
        logger.info("="*60)
        logger.info(f"ğŸš€ Executing: {self.name}")
        logger.info(f"   Script: {self.script}")
        logger.info("="*60)
        
        # æ£€æŸ¥å‰ç½®æ¡ä»¶
        ok, msg = self.check_prerequisites()
        if not ok:
            logger.error(f"âŒ Prerequisites not met: {msg}")
            self.error_msg = msg
            return False
        
        try:
            # æ‰§è¡Œè„šæœ¬
            result = subprocess.run(
                [sys.executable, self.script],
                capture_output=True,
                text=True,
                timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
            )
            
            # æ£€æŸ¥æ‰§è¡Œç»“æœ
            if result.returncode == 0:
                logger.info(f"âœ… {self.name} completed successfully")
                
                # éªŒè¯è¾“å‡ºæ–‡ä»¶
                missing_outputs = [f for f in self.outputs if not Path(f).exists()]
                if missing_outputs:
                    logger.warning(f"âš ï¸ Expected outputs not found: {missing_outputs}")
                    self.error_msg = f"Missing outputs: {missing_outputs}"
                    return False
                
                self.success = True
                return True
            else:
                logger.error(f"âŒ {self.name} failed with code {result.returncode}")
                logger.error(f"   stderr: {result.stderr[:500]}")
                self.error_msg = result.stderr
                return False
        
        except subprocess.TimeoutExpired:
            logger.error(f"âŒ {self.name} timed out after 10 minutes")
            self.error_msg = "Execution timeout"
            return False
        
        except Exception as e:
            logger.error(f"âŒ {self.name} failed with exception: {e}")
            self.error_msg = str(e)
            return False

# ============================================================
# ğŸš€ æ ¸å¿ƒ2: æ•°æ®éªŒè¯å™¨
# ============================================================
def validate_data_alignment():
    """
    éªŒè¯æ•°æ®å¯¹é½çš„æ­£ç¡®æ€§(å…³é”®è¯Šæ–­)
    """
    logger.info("="*60)
    logger.info("ğŸ” Validating data alignment...")
    logger.info("="*60)
    
    data_dir = Path("data/processed")
    
    # åŠ è½½æ–‡ä»¶
    files = {
        'queries': data_dir / "queries_final.json",
        'formulas': data_dir / "formulas.json",
        'relevance': data_dir / "relevance_labels.json"
    }
    
    data = {}
    for name, path in files.items():
        if not path.exists():
            logger.error(f"âŒ {name} file not found: {path}")
            return False
        
        with open(path, 'r', encoding='utf-8') as f:
            data[name] = json.load(f)
    
    queries = data['queries']
    formulas = data['formulas']
    relevance = data['relevance']
    
    # éªŒè¯1: æŸ¥è¯¢ä¸qrelçš„å¯¹é½
    logger.info("ğŸ“Š Check 1: Query-Qrel Alignment")
    query_ids_in_qrel = set(relevance.keys())
    query_ids_in_file = set(queries.keys())
    
    common_queries = query_ids_in_qrel & query_ids_in_file
    logger.info(f"  Queries in qrel: {len(query_ids_in_qrel)}")
    logger.info(f"  Queries in file: {len(query_ids_in_file)}")
    logger.info(f"  Common: {len(common_queries)} ({len(common_queries)/len(query_ids_in_qrel)*100:.1f}%)")
    
    if len(common_queries) == 0:
        logger.error("âŒ CRITICAL: No overlap between queries and qrel!")
        logger.error(f"   Sample qrel IDs: {list(query_ids_in_qrel)[:5]}")
        logger.error(f"   Sample query IDs: {list(query_ids_in_file)[:5]}")
        return False
    
    # éªŒè¯2: qrelä¸­çš„doc_idæ˜¯å¦åœ¨corpusä¸­
    logger.info("ğŸ“Š Check 2: Qrel-Corpus Alignment")
    all_relevant_docs = set()
    for query_rels in relevance.values():
        all_relevant_docs.update(query_rels.keys())
    
    docs_in_corpus = all_relevant_docs & set(formulas.keys())
    
    logger.info(f"  Relevant docs in qrel: {len(all_relevant_docs)}")
    logger.info(f"  Found in corpus: {len(docs_in_corpus)} ({len(docs_in_corpus)/len(all_relevant_docs)*100:.1f}%)")
    
    if len(docs_in_corpus) < len(all_relevant_docs) * 0.5:
        logger.error("âŒ CRITICAL: Less than 50% of relevant docs found in corpus!")
        
        missing_sample = list(all_relevant_docs - docs_in_corpus)[:5]
        corpus_sample = list(formulas.keys())[:5]
        
        logger.error(f"   Sample missing doc IDs: {missing_sample}")
        logger.error(f"   Sample corpus IDs: {corpus_sample}")
        logger.error("   ğŸ”§ FIX: Increase corpus_shards in prepare_final_arqmath.py")
        return False
    
    # éªŒè¯3: æŸ¥è¯¢çš„MathMLè¦†ç›–ç‡
    logger.info("ğŸ“Š Check 3: Query MathML Coverage")
    queries_with_mathml = sum(1 for q in queries.values() if q.get('mathml_skel'))
    
    logger.info(f"  Total queries: {len(queries)}")
    logger.info(f"  With MathML: {queries_with_mathml} ({queries_with_mathml/len(queries)*100:.1f}%)")
    
    if queries_with_mathml < len(queries) * 0.8:
        logger.warning("âš ï¸ WARNING: Less than 80% queries have MathML")
        logger.warning("   This may impact retrieval performance")
    
    # éªŒè¯4: LaTeXä¸MathMLçš„ä¸€è‡´æ€§(é‡‡æ ·æ£€æŸ¥)
    logger.info("ğŸ“Š Check 4: LaTeX-MathML Consistency (Sample)")
    sample_size = min(10, len(queries))
    sample_queries = list(queries.values())[:sample_size]
    
    consistent = 0
    for qdata in sample_queries:
        has_latex = bool(qdata.get('latex'))
        has_mathml = bool(qdata.get('mathml_skel'))
        
        if has_latex and has_mathml:
            consistent += 1
    
    logger.info(f"  Sample size: {sample_size}")
    logger.info(f"  With both LaTeX & MathML: {consistent}/{sample_size}")
    
    # éªŒè¯5: IDæ ¼å¼ä¸€è‡´æ€§
    logger.info("ğŸ“Š Check 5: ID Format Consistency")
    
    qrel_id_sample = list(all_relevant_docs)[:3]
    corpus_id_sample = list(formulas.keys())[:3]
    
    logger.info(f"  Sample qrel doc IDs: {qrel_id_sample}")
    logger.info(f"  Sample corpus IDs: {corpus_id_sample}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ ¼å¼å†²çª(å¦‚çº¯æ•°å­— vs å¸¦å‰ç¼€)
    qrel_numeric = all(doc_id.isdigit() for doc_id in qrel_id_sample if doc_id)
    corpus_numeric = all(fid.isdigit() for fid in corpus_id_sample if fid)
    
    if qrel_numeric != corpus_numeric:
        logger.error("âŒ CRITICAL: ID format mismatch!")
        logger.error(f"   Qrel uses numeric: {qrel_numeric}")
        logger.error(f"   Corpus uses numeric: {corpus_numeric}")
        return False
    
    logger.info("="*60)
    logger.info("âœ… All validation checks passed!")
    logger.info("="*60)
    
    # ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
    report = {
        'timestamp': datetime.now().isoformat(),
        'validation_results': {
            'query_qrel_overlap': len(common_queries) / len(query_ids_in_qrel) if query_ids_in_qrel else 0,
            'corpus_coverage': len(docs_in_corpus) / len(all_relevant_docs) if all_relevant_docs else 0,
            'query_mathml_coverage': queries_with_mathml / len(queries) if queries else 0,
            'total_queries': len(queries),
            'total_formulas': len(formulas),
            'total_relevant_pairs': sum(len(v) for v in relevance.values())
        },
        'id_samples': {
            'qrel_doc_ids': qrel_id_sample,
            'corpus_ids': corpus_id_sample
        }
    }
    
    report_file = data_dir / "validation_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2)
    
    logger.info(f"ğŸ“„ Validation report saved to {report_file}")
    
    return True

# ============================================================
# ğŸš€ æ ¸å¿ƒ3: å·¥ä½œæµç¼–æ’å™¨
# ============================================================
def run_complete_workflow():
    """
    æ‰§è¡Œå®Œæ•´çš„æ•°æ®é¢„å¤„ç†å·¥ä½œæµ
    """
    logger.info("ğŸš€ Starting complete MIR data preparation workflow...")
    logger.info(f"   Timestamp: {datetime.now().isoformat()}")
    
    # å®šä¹‰å·¥ä½œæµæ­¥éª¤
    steps = [
        WorkflowStep(
            name="Step 0: Corpus Preparation",
            script="scripts/prepare_final_arqmath.py",
            required_inputs=[
                "data/arqmath3/queries_arqmath3_task2.tsv",
                "data/arqmath3/qrel_task2_2022_official.tsv"
            ],
            outputs=[
                "data/processed/formulas.json",
                "data/processed/queries_full.json",
                "data/processed/relevance_labels.json"
            ]
        ),
        
        WorkflowStep(
            name="Step 1: Extract Query MathML from XML",
            script="scripts/extract_query_mathml_from_xml.py",
            required_inputs=[
                "data/arqmath3/Topics_Task2_2022_V0.1.xml",
                "data/processed/queries_full.json"
            ],
            outputs=[
                "data/processed/queries_full_with_mathml.json"
            ]
        ),
        
        WorkflowStep(
            name="Step 2: Supplement Missing MathML",
            script="scripts/fix_query_mathml_matching.py",
            required_inputs=[
                "data/processed/queries_full_with_mathml.json",
                "data/processed/formulas.json"
            ],
            outputs=[
                "data/processed/queries_final.json"
            ]
        )
    ]
    
    # æ‰§è¡Œæ­¥éª¤
    results = []
    for step in steps:
        success = step.execute()
        results.append({
            'step': step.name,
            'success': success,
            'error': step.error_msg
        })
        
        if not success:
            logger.error(f"âŒ Workflow stopped at: {step.name}")
            break
    
    # å¦‚æœæ‰€æœ‰æ­¥éª¤æˆåŠŸ,æ‰§è¡ŒéªŒè¯
    all_success = all(r['success'] for r in results)
    
    if all_success:
        logger.info("ğŸ‰ All preprocessing steps completed!")
        logger.info("ğŸ” Running final validation...")
        
        validation_ok = validate_data_alignment()
        
        if validation_ok:
            logger.info("="*60)
            logger.info("âœ… WORKFLOW COMPLETED SUCCESSFULLY!")
            logger.info("   Data is ready for indexing and evaluation")
            logger.info("="*60)
            logger.info("ğŸ“‹ Next steps:")
            logger.info("   1. python scripts/build_index.py")
            logger.info("   2. python scripts/build_graph.py")
            logger.info("   3. python scripts/train_ranker.py")
            logger.info("   4. python scripts/run_eval.py")
            logger.info("="*60)
        else:
            logger.error("âŒ Validation failed. Please review the errors above.")
    else:
        logger.error("âŒ Workflow failed. Check logs above for details.")
    
    # ä¿å­˜å·¥ä½œæµæŠ¥å‘Š
    workflow_report = {
        'timestamp': datetime.now().isoformat(),
        'steps': results,
        'overall_success': all_success
    }
    
    report_file = Path("data/processed/workflow_report.json")
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(workflow_report, f, indent=2)
    
    logger.info(f"ğŸ“„ Workflow report saved to {report_file}")
    
    return all_success

# ============================================================
# ä¸»å…¥å£
# ============================================================
if __name__ == "__main__":
    success = run_complete_workflow()
    sys.exit(0 if success else 1)