# import json
# import torch
# import faiss
# import sys
# from transformers import AutoTokenizer, AutoModel
# from pathlib import Path
# from tqdm import tqdm

# def check_top_results():
#     DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#     MODEL_NAME = "witiko/mathberta"
    
#     print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–è¯­ä¹‰æ£€æŸ¥å·¥å…·...")

#     # 1. åŠ è½½æ˜ å°„è¡¨ (è¿™ä¸ªæ¯”è¾ƒå°ï¼Œå¾ˆå¿«)
#     mapping_path = Path("artifacts/vector_id_mapping_pq.json")
#     if not mapping_path.exists():
#         print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ˜ å°„æ–‡ä»¶ {mapping_path}")
#         return
#     with open(mapping_path, 'r') as f:
#         fids = json.load(f)
#     print(f"âœ… å·²åŠ è½½æ˜ å°„è¡¨ï¼ŒåŒ…å« {len(fids):,} æ¡å…¬å¼ ID")

#     # 2. åŠ è½½ Faiss ç´¢å¼•
#     index_path = Path("artifacts/vector_index_pq.faiss")
#     print(f"ğŸ“¦ æ­£åœ¨åŠ è½½ Faiss ç´¢å¼• ({index_path.stat().st_size / 1024**2:.2f} MB)...")
#     index = faiss.read_index(str(index_path))
#     print("âœ… ç´¢å¼•åŠ è½½å®Œæˆ")

#     # 3. åŠ è½½ MathBERT æ¨¡å‹
#     print(f"ğŸ¤– æ­£åœ¨åŠ è½½æ¨¡å‹ {MODEL_NAME} åˆ° {DEVICE}...")
#     tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
#     model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)
#     model.eval()
#     print("âœ… æ¨¡å‹å‡†å¤‡å°±ç»ª")

#     # 4. ä¼˜åŒ–åŠ è½½ formulas.json (å…³é”®æ­¥éª¤)
#     print("ğŸ“– æ­£åœ¨è¯»å–å…¬å¼å…ƒæ•°æ® (ä»…è¯»å–å‰ 100,000 æ¡ä»¥èŠ‚çœå†…å­˜)...")
#     corpus = {}
#     with open("data/processed/formulas.json", 'r', encoding='utf-8') as f:
#         # ä½¿ç”¨æµå¼æ€è·¯æ¨¡æ‹Ÿè¿›åº¦æ¡ï¼ˆè™½ç„¶ json.load æ˜¯é˜»å¡çš„ï¼Œä½†æˆ‘ä»¬å¯ä»¥å…ˆè¯»å–å‰ 100k æ¡å¯¹åº”çš„å…ƒæ•°æ®ï¼‰
#         # å¦‚æœä½ ä¹‹å‰åªç´¢å¼•äº† 10w æ¡ï¼Œè¿™é‡Œæˆ‘ä»¬ä¹Ÿåªå–å‰ 10w æ¡
#         full_corpus = json.load(f)
#         for i, fid in enumerate(tqdm(fids, desc="æ˜ å°„å…ƒæ•°æ®")):
#             if fid in full_corpus:
#                 corpus[fid] = full_corpus[fid]
#         del full_corpus # ç«‹å³é‡Šæ”¾å…¨é‡å¤§å­—å…¸

#     # 5. å‡†å¤‡æŸ¥è¯¢
#     with open("data/processed/queries_full.json", 'r') as f:
#         queries = json.load(f)
    
#     # æŒ‘é€‰ç¬¬ 50 ä¸ªæŸ¥è¯¢ï¼ˆé¿å¼€æœ€ç®€å•çš„ï¼Œæ‰¾ä¸ªæœ‰æŒ‘æˆ˜æ€§çš„ï¼‰
#     test_qid = list(queries.keys())[50] 
#     query_latex = queries[test_qid]['latex_norm']
    
#     print("-" * 50)
#     print(f"ğŸ” [æŸ¥è¯¢ä¸»é¢˜]: {test_qid}")
#     print(f"ğŸ” [æŸ¥è¯¢å…¬å¼]: {query_latex}")
#     print("-" * 50)
    
#     # 6. æ‰§è¡Œå‘é‡ç¼–ç ä¸æ£€ç´¢
#     print("ğŸ§  æ­£åœ¨è¿›è¡Œæ·±åº¦è¯­ä¹‰ç¼–ç ...")
#     inputs = tokenizer([query_latex], padding=True, truncation=True, max_length=128, return_tensors="pt").to(DEVICE)
#     with torch.no_grad():
#         q_emb = model(**inputs).last_hidden_state[:, 0, :].cpu().numpy()
#     faiss.normalize_L2(q_emb)
    
#     print("ğŸ” æ­£åœ¨è¯­ä¹‰ç©ºé—´æœç´¢ Top 5...")
#     D, I = index.search(q_emb, 5)
    
#     print("\nğŸ¯ [è¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢ç»“æœ]:")
#     for i, idx in enumerate(I[0]):
#         if idx == -1: continue
#         fid = fids[idx]
#         score = D[0][i]
#         res_latex = corpus.get(fid, {}).get('latex_norm', "æœªçŸ¥å†…å®¹")
#         print(f"Rank {i+1} [ç›¸ä¼¼åº¦: {score:.4f}]:")
#         print(f"   ID: {fid}")
#         print(f"   LaTeX: {res_latex}")
#         print("-" * 20)

# if __name__ == "__main__":
#     check_top_results()
import json
import torch
import faiss
from transformers import AutoTokenizer, AutoModel
from pathlib import Path

def get_latex_robust(target_ids, json_path):
    """
    æœ€ç¨³å¥çš„æµå¼ ID æŸ¥æ‰¾ï¼šé€è¡Œè¯»å–å¹¶è§£æ JSON é€»è¾‘å—
    """
    results = {}
    target_ids = {str(tid) for tid in target_ids}
    print(f"ğŸ“– æ­£åœ¨å…¨é‡æ‰«æ 1300 ä¸‡æ¡å…ƒæ•°æ®ï¼Œå¯»æ‰¾ ID: {target_ids}")
    
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            current_id = None
            for line in f:
                # åŒ¹é…è¡Œå¦‚: "2742591": {
                line = line.strip()
                for fid in list(target_ids):
                    if line.startswith(f'"{fid}":'):
                        current_id = fid
                        break
                
                # å¦‚æœå½“å‰è¡Œåœ¨æŸä¸ªéœ€è¦çš„ ID å—å†…ï¼Œå¯»æ‰¾ latex_norm
                if current_id and '"latex_norm":' in line:
                    # æå–å¼•å·å†…çš„å†…å®¹
                    start = line.find('": "') + 4
                    end = line.rfind('"')
                    if start > 3 and end > start:
                        results[current_id] = line[start:end]
                        target_ids.remove(current_id)
                        current_id = None
                
                if not target_ids:
                    break
    except Exception as e:
        print(f"âŒ æ‰«æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    return results

def check_semantic():
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    MODEL_NAME = "witiko/mathberta"
    
    # 1. åŠ è½½æ˜ å°„ä¸ç´¢å¼•
    with open("artifacts/vector_id_mapping_pq.json", 'r') as f:
        fids = json.load(f)
    index = faiss.read_index("artifacts/vector_index_pq.faiss")
    
    # 2. åŠ è½½æ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)
    model.eval()

    # 3. æ‰§è¡ŒæŸ¥è¯¢ (å‹¾è‚¡å®šç†)
    # query_latex = r"a^2 + b^2 = c^2"
    query_latex = r"x^2 + y^2 = z^2"
    print(f"\nğŸ” æŸ¥è¯¢å…¬å¼: {query_latex}")

    inputs = tokenizer([query_latex], padding=True, truncation=True, max_length=128, return_tensors="pt").to(DEVICE)
    with torch.no_grad():
        q_emb = model(**inputs).last_hidden_state[:, 0, :].cpu().numpy()
    faiss.normalize_L2(q_emb)
    
    D, I = index.search(q_emb, 5)
    result_ids = [fids[idx] for idx in I[0] if idx != -1]

    # 4. é²æ£’æå–å†…å®¹
    content_map = get_latex_robust(result_ids, "data/processed/formulas.json")

    print("\nğŸ¯ [è¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢ç»“æœ]:")
    for i, idx in enumerate(I[0]):
        fid = fids[idx]
        latex = content_map.get(str(fid), "å†…å®¹æå–å¤±è´¥")
        print(f"Rank {i+1} [ç›¸ä¼¼åº¦: {D[0][i]:.4f}]:")
        print(f"   ID: {fid}")
        print(f"   LaTeX: {latex}")
        print("-" * 20)

if __name__ == "__main__":
    check_semantic()