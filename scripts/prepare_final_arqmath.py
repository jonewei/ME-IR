import json
import csv
import sys
import re
import pickle
from pathlib import Path
from tqdm import tqdm

# ç¡®ä¿å¯¼å…¥è·¯å¾„
sys.path.append(str(Path(__file__).resolve().parent.parent))
from retrieval.approach0_hash import DualHashGenerator, Approach0HashIndex

csv.field_size_limit(sys.maxsize)

# =========================== æ ¸å¿ƒé€»è¾‘ï¼šå¤„ç†æŸ¥è¯¢ TSV ===========================
def process_queries(base_path, hash_gen):
    """ç›´æ¥ä»å®˜æ–¹ TSV æå– Task 2 çš„æŸ¥è¯¢å…¬å¼"""
    tsv_path = base_path / "data" / "arqmath3" / "queries_arqmath3_task2.tsv"
    out_path = base_path / "data" / "processed" / "queries_full.json"
    out_path.parent.mkdir(exist_ok=True, parents=True)
    
    queries = {}
    print(f"\nğŸ” æ­£åœ¨ä» TSV æå–æŸ¥è¯¢å…¬å¼...")
    if not tsv_path.exists():
        print(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ° {tsv_path}ï¼Œè¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„ï¼")
        return

    with open(tsv_path, 'r', encoding='utf-8') as f:
        reader = csv.reader(f, delimiter='\t')
        for row in reader:
            if len(row) >= 2:
                topic_id = row[0].strip()
                raw_latex = row[1].strip()
                # ç»Ÿä¸€ä½¿ç”¨ DualHashGenerator çš„æ¸…æ´—é€»è¾‘
                queries[topic_id] = hash_gen.clean_latex(raw_latex)

    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(queries, f, ensure_ascii=False, indent=2)
    print(f"âœ… æŸ¥è¯¢é›†å·²å°±ç»ª: {len(queries)} æ¡ -> {out_path}")

# =========================== æ ¸å¿ƒé€»è¾‘ï¼šè¯­æ–™å¤„ç† (Visual ID å¯¹é½) ===========================
def process_corpus(num_shards=101):
    base_path = Path.cwd()
    latex_dir = base_path / "data" / "arqmath3" / "latex_representation_v3"
    
    hash_gen = DualHashGenerator()
    h_index = Approach0HashIndex()
    
    # 1. å…ˆå¤„ç†æŸ¥è¯¢
    process_queries(base_path, hash_gen)
    
    # 2. å­˜å‚¨å…ƒæ•°æ®ï¼škey å¿…é¡»æ˜¯ visual_id
    corpus = {} 
    
    latex_files = sorted(latex_dir.glob("*.tsv"))[:num_shards]
    print(f"\nğŸ”„ æ­£åœ¨å¤„ç† {len(latex_files)} ä¸ªè¯­æ–™åˆ†ç‰‡...")
    
    for f in tqdm(latex_files, desc="Processing Shards"):
        with open(f, 'r', encoding='utf-8') as fin:
            reader = csv.reader(fin, delimiter='\t')
            next(reader, None)  # è·³è¿‡è¡¨å¤´
            
            for row in reader:
                if len(row) < 9: continue
                
                # README ç»“æ„: [0:id, 6:visual_id, 7:issue, 8:formula]
                visual_id = row[6].strip()
                issue = row[7].strip()
                raw_latex = row[8].strip()
                
                # è¿‡æ»¤ 'd' (ä¸å­˜åœ¨äºXML)
                if 'd' in issue: continue
                
                # Visual ID å»é‡ (åŒä¸€å…¬å¼åªç´¢å¼•ä¸€æ¬¡)
                if visual_id in corpus: continue
                
                clean_norm = hash_gen.clean_latex(raw_latex)
                
                corpus[visual_id] = {
                    "formula_id": visual_id,
                    "latex": raw_latex,
                    "latex_norm": clean_norm
                }
                
                # æ„å»ºå“ˆå¸Œç´¢å¼•
                h_val = hash_gen.generate_latex_hash(clean_norm)
                if h_val not in h_index.index:
                    h_index.index[h_val] = []
                h_index.index[h_val].append(visual_id)

    # 3. å¯¼å‡º
    out_dir = base_path / "data" / "processed"
    out_dir.mkdir(exist_ok=True, parents=True)
    
    print("\nğŸ’¾ æ­£åœ¨å¯¼å‡ºå¯¹é½åçš„ç´¢å¼•æ•°æ®...")
    with open(out_dir / "formulas.json", 'w', encoding='utf-8') as f:
        json.dump(corpus, f, ensure_ascii=False)

    h_index.save(base_path / "artifacts" / "approach0_index.pkl")
    
    print(f"âœ… å¤„ç†å®Œæˆï¼")
    print(f"   - å”¯ä¸€ Visual ID æ•°é‡: {len(corpus):,}")
    print(f"   - è¯­æ–™å…ƒæ•°æ® -> {out_dir}/formulas.json")
    print(f"   - å“ˆå¸Œç´¢å¼• -> artifacts/approach0_index.pkl")

if __name__ == "__main__":
    # æ‰§è¡Œå…¨æµç¨‹
    process_corpus(num_shards=101) # ä¹Ÿå¯ä»¥ç›´æ¥æ”¹ä¸º 101