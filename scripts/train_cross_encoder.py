import json
import os
from torch.utils.data import DataLoader
from sentence_transformers import InputExample, CrossEncoder
import math

# 1. é…ç½®å‚æ•° (å»ºè®®ä½¿ç”¨ç»å¯¹è·¯å¾„)
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
TRAIN_DATA_PATH = os.path.join(PROJECT_ROOT, "data/train_cross_encoder.jsonl")
MODEL_OUTPUT_PATH = os.path.join(PROJECT_ROOT, "artifacts/cross_encoder_model")
# BASE_MODEL = "bert-base-uncased" 
# BATCH_SIZE = 64 # 4090 æ˜¾å­˜å¤§ï¼Œç›´æ¥ä¸Š 64 æé€Ÿ
# NUM_EPOCHS = 3
# LEARNING_RATE = 2e-5
BASE_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2" # æ¢æˆç²¾æ’ä¸“ç”¨æ¨¡å‹
NUM_EPOCHS = 10 # å¢åŠ è½®æ•°
LEARNING_RATE = 5e-6 # é™ä½å­¦ä¹ ç‡
BATCH_SIZE = 64 # 4090 æ˜¾å­˜å¤§ï¼Œç›´æ¥ä¸Š 64 æé€Ÿ

def extract_latex(item):
    if isinstance(item, str): return item
    if isinstance(item, dict): return item.get("latex_norm") or item.get("latex") or ""
    return str(item)

def train():
    # --- å…³é”®ä¿®å¤ï¼šç¡®ä¿ç›®å½•å­˜åœ¨ ---
    if not os.path.exists(MODEL_OUTPUT_PATH):
        os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)
        print(f"ğŸ“ å·²åˆ›å»ºè¾“å‡ºç›®å½•: {MODEL_OUTPUT_PATH}")

    print(f"ğŸš€ é‡æ–°å¯åŠ¨è®­ç»ƒ (åŸºäº {BASE_MODEL})...")
    
    # åŠ è½½æ•°æ®é€»è¾‘ (ä¿æŒä¸å˜)
    train_examples = []
    with open(TRAIN_DATA_PATH, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            q = extract_latex(data['texts'][0])
            d = extract_latex(data['texts'][1])
            train_examples.append(InputExample(texts=[q, d], label=float(data['label'])))
    
    print(f"ğŸ“¦ æœ‰æ•ˆæ•°æ®é‡: {len(train_examples)}")

    model = CrossEncoder(BASE_MODEL, num_labels=1, device="cuda")
    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE)
    warmup_steps = math.ceil(len(train_dataloader) * NUM_EPOCHS * 0.1)

    # å¼€å§‹è®­ç»ƒ
    model.fit(
        train_dataloader=train_dataloader,
        epochs=NUM_EPOCHS,
        optimizer_params={'lr': LEARNING_RATE},
        warmup_steps=warmup_steps,
        output_path=MODEL_OUTPUT_PATH, # è™½ç„¶ fit ä¼šå­˜ï¼Œä½†æœ‰æ—¶ä¼šå› å¼‚å¸¸è·³è¿‡
        show_progress_bar=True
    )

    # --- å…³é”®ä¿®å¤ï¼šæ˜¾å¼å¼ºåˆ¶ä¿å­˜ ---
    print("ğŸ’¾ æ­£åœ¨æ‰§è¡Œæ˜¾å¼ä¿å­˜...")
    model.save(MODEL_OUTPUT_PATH)
    model.tokenizer.save_pretrained(MODEL_OUTPUT_PATH)
    
    # æ£€æŸ¥æ˜¯å¦çœŸçš„å­˜ä¸Šäº†
    if os.path.exists(os.path.join(MODEL_OUTPUT_PATH, "config.json")):
        print(f"âœ… éªŒè¯æˆåŠŸï¼æ¨¡å‹å·²è½åœ°: {MODEL_OUTPUT_PATH}")
    else:
        print("âŒ è­¦å‘Šï¼šä¿å­˜åŠ¨ä½œæ‰§è¡Œäº†ï¼Œä½† config.json ä»ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ç£ç›˜ç©ºé—´ï¼")

if __name__ == "__main__":
    train()