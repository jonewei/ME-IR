import json
import torch
from transformers import AutoTokenizer, AutoModel
import numpy as np
from tqdm import tqdm
from scipy.spatial.distance import cosine

# --- é…ç½®åŒº ---
MODEL_NAME = "math-similarity/Bert-MLM_arXiv-MP-class_zbMath"
RELEVANCE_PATH = "data/processed/relevance_labels.json"
QUERY_PATH = "data/processed/queries_full.json"
CORPUS_PATH = "data/processed/formulas.json"

# --- 1. åŠ è½½æ¨¡å‹ï¼ˆæ•°å­¦ä¸“ç”¨ç‰ˆï¼‰ ---
print(f"ğŸ“¡ æ­£åœ¨åŠ è½½æ•°å­¦ä¸“å®¶æ¨¡å‹: {MODEL_NAME}...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(MODEL_NAME)

def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    # å– [CLS] å‘é‡ä½œä¸ºè¡¨å¾
    return outputs.last_hidden_state[0][0].numpy()

# --- 2. åŠ è½½å®éªŒæ•°æ® ---
with open(RELEVANCE_PATH, 'r') as f: relevance = json.load(f)
with open(QUERY_PATH, 'r') as f: queries = json.load(f)
with open(CORPUS_PATH, 'r') as f: corpus = json.load(f)

# --- 3. æ ¸å¿ƒå®éªŒï¼šé’ˆå¯¹ 76 æ¡ Query è¿›è¡Œè¯­ä¹‰æ•æ„Ÿåº¦æµ‹è¯• ---
print("ğŸ§ª å¼€å§‹æ•°å­¦è¯­ä¹‰å¯¹æ ‡æµ‹è¯• (Sampled Reranking)...")

results = []
# ä¸ºäº†å¿«é€ŸéªŒè¯ï¼Œæˆ‘ä»¬é€‰å–ä½ ä¹‹å‰è¯„ä¼°è¿‡çš„ test_qids
for qid in tqdm(list(relevance.keys())[:20]):  # å…ˆæµ‹20æ¡çœ‹è¶‹åŠ¿
    q_latex = queries[qid]
    gt_id = list(relevance[qid].keys())[0]  # è·å–çœŸå€¼ID
    gt_latex = corpus[str(gt_id)]['latex_norm']
    
    # è·å–æ•°å­¦ BERT çš„ç¼–ç 
    q_vec = get_embedding(q_latex)
    gt_vec = get_embedding(gt_latex)
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    sim_score = 1 - cosine(q_vec, gt_vec)
    results.append(sim_score)

print(f"\nâœ… å®éªŒå®Œæˆï¼")
print(f"ğŸ“Š Math-BERT å¯¹çœŸå€¼å…¬å¼çš„å¹³å‡è¯­ä¹‰ç›¸ä¼¼åº¦ (Similarity Score): {np.mean(results):.4f}")