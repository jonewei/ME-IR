import json
import os
import torch
import numpy as np
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer, util

# --- è·¯å¾„é…ç½® ---
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
QUERY_PATH = os.path.join(PROJECT_ROOT, "data/processed/queries_full.json")
CORPUS_PATH = os.path.join(PROJECT_ROOT, "data/processed/formulas.json")
RELEVANCE_PATH = os.path.join(PROJECT_ROOT, "data/processed/relevance_labels.json")

# --- æ¨¡å‹é…ç½® ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MINILM_NAME = "sentence-transformers/all-MiniLM-L6-v2"
MATHBERT_NAME = "math-similarity/Bert-MLM_arXiv-MP-class_zbMath"

class BenchmarkEvaluator:
    def __init__(self):
        print(f"ğŸ“¡ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹ (Device: {DEVICE})...")
        # 1. åŠ è½½ MiniLM
        self.minilm = SentenceTransformer(MINILM_NAME).to(DEVICE)
        # 2. åŠ è½½ Math-BERT
        self.math_tokenizer = AutoTokenizer.from_pretrained(MATHBERT_NAME)
        self.math_model = AutoModel.from_pretrained(MATHBERT_NAME).to(DEVICE)
        
        # 3. åŠ è½½æ•°æ®
        with open(QUERY_PATH, 'r') as f: self.queries = json.load(f)
        with open(RELEVANCE_PATH, 'r') as f: self.relevance = json.load(f)
        with open(CORPUS_PATH, 'r') as f: self.corpus = json.load(f)
        
        # ç­›é€‰ 76 æ¡æœ‰æ•ˆæŸ¥è¯¢
        self.test_qids = [qid for qid in self.queries.keys() if qid in self.relevance]
        print(f"âœ… æ•°æ®å‡†å¤‡å°±ç»ªï¼Œå…±è®¡ {len(self.test_qids)} æ¡æœ‰æ•ˆéªŒè¯æŸ¥è¯¢ã€‚")

    def get_mathbert_embedding(self, text):
        inputs = self.math_tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True).to(DEVICE)
        with torch.no_grad():
            outputs = self.math_model(**inputs)
        return outputs.last_hidden_state[0][0] # CLS token

    def run_comparison(self):
        mrr_minilm = []
        mrr_mathbert = []
        
        # ä¸ºäº†å…¬å¹³å¯¹æ¯”ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç»„â€œå€™é€‰æ± â€ã€‚
        # å®é™…è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å– Hybrid æœç´¢å‡ºçš„ Top-100 è¿›è¡Œé‡æ’ï¼Œçœ‹è¯­ä¹‰æ¨¡å‹èƒ½å¦è‡ªæˆ‘ä¿®æ­£ã€‚
        # è¿™é‡Œæ¨¡æ‹Ÿæ¯ä¸ª Query çš„é‡æ’è¿‡ç¨‹
        for qid in tqdm(self.test_qids, desc="Benchmarking Models"):
            q_latex = self.queries[qid]
            gt_fids = list(self.relevance[qid].keys()) # è·å–æ‰€æœ‰çœŸå€¼ ID
            
            # 1. æ¨¡æ‹Ÿä¸€ä¸ªå€™é€‰æ±  (åŒ…å«çœŸå€¼ + 99ä¸ªè´Ÿæ ·æœ¬)
            # æ³¨æ„ï¼šåœ¨æ­£å¼å®éªŒä¸­ï¼Œå€™é€‰æ± åº”æ¥è‡ªæ£€ç´¢å™¨çš„åˆæ­¥å¬å›ç»“æœ
            candidate_fids = gt_fids + list(self.corpus.keys())[:99] 
            candidate_texts = [self.corpus[str(fid)]['latex_norm'] for fid in candidate_fids]
            
            # --- MiniLM æ’åº ---
            q_emb_mini = self.minilm.encode(q_latex, convert_to_tensor=True)
            c_emb_mini = self.minilm.encode(candidate_texts, convert_to_tensor=True)
            scores_mini = util.cos_sim(q_emb_mini, c_emb_mini)[0].cpu().numpy()
            
            # --- Math-BERT æ’åº ---
            q_emb_math = self.get_mathbert_embedding(q_latex)
            c_embs_math = torch.stack([self.get_mathbert_embedding(t) for t in candidate_texts])
            scores_math = util.cos_sim(q_emb_math, c_embs_math)[0].cpu().numpy()
            
            # è®¡ç®—æ’åå’Œ MRR (çœŸå€¼ ID åœ¨åˆ—è¡¨å‰éƒ¨ï¼Œå³ç´¢å¼• 0)
            def get_mrr(scores):
                # å¯¹åˆ†æ•°é™åºæ’åˆ—ï¼Œè·å–åŸå§‹ç´¢å¼•
                ranked_idx = np.argsort(scores)[::-1]
                # æ‰¾åˆ°çœŸå€¼ï¼ˆç´¢å¼•ä¸º0ï¼‰åœ¨æ’åºåçš„ä½ç½®
                rank = np.where(ranked_idx == 0)[0][0] + 1
                return 1.0 / rank

            mrr_minilm.append(get_mrr(scores_mini))
            mrr_mathbert.append(get_mrr(scores_math))

        print("\n" + "="*40)
        print(f"ğŸ† è¯­ä¹‰å¯¹æ ‡å®éªŒç»“æœ (N={len(self.test_qids)})")
        print("-"*40)
        print(f"ğŸ”¹ MiniLM (General) MRR:    {np.mean(mrr_minilm):.4f}")
        print(f"ğŸ”¹ Math-BERT (Domain) MRR:  {np.mean(mrr_mathbert):.4f}")
        print(f"â­ Hybrid (Proposed) MRR:  0.8062 (From Final Eval)")
        print("="*40)
        print("\nğŸ’¡ ç»“è®ºï¼šå¦‚æœ Math-BERT çš„ MRR ä½äº Hybridï¼Œåˆ™è¯æ˜ç»“æ„ç‰¹å¾æ˜¯ä¸å¯æˆ–ç¼ºçš„ã€‚")

if __name__ == "__main__":
    evaluator = BenchmarkEvaluator()
    evaluator.run_comparison()