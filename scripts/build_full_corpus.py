import json
import csv
import sys
import pickle
from pathlib import Path
from tqdm import tqdm

# å¼ºåˆ¶å¼•å…¥
sys.path.append(str(Path(__file__).resolve().parent.parent))
from retrieval.approach0_hash import DualHashGenerator, Approach0HashIndex

csv.field_size_limit(sys.maxsize)

def build_full_system():
    base_path = Path.cwd()
    latex_dir = base_path / "data" / "arqmath3" / "latex_representation_v3"
    out_dir = base_path / "data" / "processed"
    artifact_dir = base_path / "artifacts"
    
    out_dir.mkdir(exist_ok=True, parents=True)
    artifact_dir.mkdir(exist_ok=True)

    hash_gen = DualHashGenerator()
    
    # --- Part 1: å¤„ç†æŸ¥è¯¢ ---
    print("ğŸ” æ­£åœ¨æå–æŸ¥è¯¢é›†...")
    queries = {}
    q_tsv = base_path / "data" / "arqmath3" / "queries_arqmath3_task2.tsv"
    with open(q_tsv, 'r', encoding='utf-8') as f:
        reader = csv.reader(f, delimiter='\t')
        for row in reader:
            if len(row) >= 2:
                q_norm, _ = hash_gen.clean_latex(row[1].strip())
                queries[row[0].strip()] = q_norm
    
    query_file = out_dir / "queries_full.json"
    with open(query_file, 'w', encoding='utf-8') as f:
        json.dump(queries, f, ensure_ascii=False, indent=2)

    # --- Part 2: å¤„ç†è¯­æ–™ ---
    # è‡ªåŠ¨è¯†åˆ«ç›®å½•ä¸‹æ‰€æœ‰çš„ TSV åˆ†ç‰‡
    all_shards = sorted(list(latex_dir.glob("*.tsv")))
    corpus = {}
    h_index = Approach0HashIndex()
    
    # è¯¦ç»†ç»Ÿè®¡æŒ‡æ ‡
    stats = {
        "total_instances": 0,    # è¯»å–çš„æ€»è¡Œæ•°
        "skipped_issue_d": 0,   # å®˜æ–¹æ ‡è®°æ— æ•ˆçš„
        "duplicate_skipped": 0, # å›  Visual ID é‡å¤è€Œè·³è¿‡çš„
        "unique_visual_ids": 0, # æœ€ç»ˆå…¥åº“çš„å”¯ä¸€ ID
        "normalized_count": 0   # è§¦å‘å¢å¼ºæ¸…æ´—è§„åˆ™çš„
    }

    print(f"ğŸš€ å¯åŠ¨æ‰«æã€‚å‘ç°åˆ†ç‰‡æ€»æ•°: {len(all_shards)}")
    # å¦‚æœæƒ³è·‘å…¨é‡ï¼Œä¸è¦åˆ‡ç‰‡ï¼›å¦‚æœæƒ³å…ˆæµ‹è¯•ï¼Œå¯ä»¥ç”¨ all_shards[:101]
    for shard in tqdm(all_shards, desc="Processing Shards"):
        with open(shard, 'r', encoding='utf-8') as fin:
            reader = csv.reader(fin, delimiter='\t')
            next(reader, None) # è·³è¿‡è¡¨å¤´
            for row in reader:
                if len(row) < 9: continue
                stats["total_instances"] += 1
                
                visual_id = row[6].strip()
                issue = row[7].strip()
                raw_latex = row[8].strip()
                
                # è¿‡æ»¤é€»è¾‘ 1: æ— æ•ˆå…¬å¼
                if 'd' in issue:
                    stats["skipped_issue_d"] += 1
                    continue
                
                # è¿‡æ»¤é€»è¾‘ 2: é‡å¤ Visual ID (æ ¸å¿ƒå»é‡ç‚¹)
                if visual_id in corpus:
                    stats["duplicate_skipped"] += 1
                    continue
                
                # æ‰§è¡Œæ¸…æ´—
                norm_latex, was_norm = hash_gen.clean_latex(raw_latex)
                if was_norm: stats["normalized_count"] += 1
                
                # å…¥åº“
                corpus[visual_id] = {
                    "formula_id": visual_id,
                    "latex": raw_latex,
                    "latex_norm": norm_latex
                }
                
                # ç´¢å¼•å“ˆå¸Œ
                h_val = hash_gen.generate_latex_hash(norm_latex)
                if h_val not in h_index.index:
                    h_index.index[h_val] = []
                h_index.index[h_val].append(visual_id)
                
                stats["unique_visual_ids"] += 1

    # --- Part 3: ä¿å­˜ä¸æ±‡æ€» ---
    print("\nğŸ’¾ æ­£åœ¨ä¿å­˜ç´¢å¼•æ–‡ä»¶...")
    corpus_file = out_dir / "formulas.json"
    index_file = artifact_dir / "approach0_index.pkl"
    
    with open(corpus_file, 'w', encoding='utf-8') as f:
        json.dump(corpus, f, ensure_ascii=False)
    h_index.save(index_file)
    
    print("\n" + "="*50)
    print("ğŸ“Š æœ€ç»ˆæ„å»ºæ±‡æ€»æŠ¥å‘Š")
    print("="*50)
    print(f"1. åŸå§‹å®ä¾‹æ€»æ•° (Instances):   {stats['total_instances']:,}")
    print(f"2. æ— æ•ˆæ•°æ®è¿‡æ»¤ (Issue 'd'):  {stats['skipped_issue_d']:,}")
    print(f"3. é‡å¤å…¬å¼è¿‡æ»¤ (Duplicates): {stats['duplicate_skipped']:,}")
    print(f"4. å”¯ä¸€ Visual ID (Index Size): {stats['unique_visual_ids']:,}")
    print(f"   (å»é‡ç‡: {stats['duplicate_skipped']/max(1, stats['total_instances'])*100:.2f}%)")
    print(f"5. ç¬¦å·è§„èŒƒåŒ–å‘½ä¸­æ¬¡æ•°:         {stats['normalized_count']:,}")
    print("-" * 50)
    print("ğŸ“ å·²ç”Ÿæˆæ–‡ä»¶æ¸…å•:")
    print(f"   - æŸ¥è¯¢é›† JSON:  {query_file}")
    print(f"   - è¯­æ–™å…ƒæ•°æ®:   {corpus_file}")
    print(f"   - å“ˆå¸Œè·¯ç´¢å¼•:   {index_file}")
    print("="*50)
    print("ğŸ’¡ æç¤ºï¼šå¦‚æœå”¯ä¸€ ID æ•°è¿œä½äº 2800 ä¸‡ï¼Œè¯·ç¡®è®¤æ˜¯å¦æ‰«æäº†å…¨éƒ¨ 300+ ä¸ªåˆ†ç‰‡ã€‚")

if __name__ == "__main__":
    build_full_system()