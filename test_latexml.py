import json
import csv
import sys
import re
import pickle
from pathlib import Path
from tqdm import tqdm
from collections import defaultdict

# ç¡®ä¿å¯¼å…¥è·¯å¾„
sys.path.append(str(Path(__file__).resolve().parent.parent))
from retrieval.approach0_hash import DualHashGenerator, Approach0HashIndex

csv.field_size_limit(sys.maxsize)

# ===========================  ARQMath ä¼˜åŒ–ç‰ˆç¬¦å·æ˜ å°„ ===========================
ARQMATH_SYMBOL_MAPPING = {
    # åˆ†éš”ç¬¦ç­‰ä»·
    r'\|': '||',
    r'\Vert': '||',
    r'\lbrace': '{',
    r'\rbrace': '}',
    r'\langle': '<',
    r'\rangle': '>',
    
    # å¸Œè…Šå­—æ¯å˜ä½“ï¼ˆä¿å®ˆåˆå¹¶ï¼‰
    r'\varepsilon': r'\epsilon',
    r'\varphi': r'\phi',
    # æ³¨æ„ï¼šä¿ç•™ \vartheta, \varpi ç­‰ï¼Œå®ƒä»¬åœ¨æŸäº›æ•°å­¦é¢†åŸŸæœ‰ç‰¹æ®Šå«ä¹‰
    
    # å…³ç³»ç¬¦å·
    r'\le': r'\leq',
    r'\ge': r'\geq',
    r'\ne': r'\neq',
    
    # ç®­å¤´ç¬¦å·
    r'\to': r'\rightarrow',
    r'\gets': r'\leftarrow',
    r'\implies': r'\Rightarrow',
    r'\iff': r'\Leftrightarrow',
    
    # é€»è¾‘ç¬¦å·
    r'\land': r'\wedge',
    r'\lor': r'\vee',
    r'\lnot': r'\neg',
    
    # é›†åˆè®º
    r'\empty': r'\emptyset',
    r'\varnothing': r'\emptyset',
    
    # è½¬ç½®ç¬¦å·ï¼ˆä¿å®ˆå¤„ç†ï¼‰
    r'^\top': '^T',
    r'^t': '^T',
    
    # çœç•¥å·
    r'\ldots': r'\cdots',
    r'\dots': r'\cdots',
}

class ARQMathDualHashGenerator(DualHashGenerator):
    """ARQMath-3 ç‰¹åŒ–çš„å“ˆå¸Œç”Ÿæˆå™¨"""
    
    def __init__(self):
        super().__init__()
        # ä½¿ç”¨ ARQMath ä¼˜åŒ–çš„æ˜ å°„è¡¨
        self.sorted_symbols = sorted(
            ARQMATH_SYMBOL_MAPPING.items(), 
            key=lambda x: len(x[0]), 
            reverse=True
        )
        
        # åªç§»é™¤è¿™äº›å­—ä½“å‘½ä»¤ï¼ˆä¿ç•™ \mathbb, \mathcal å› ä¸ºæœ‰è¯­ä¹‰ï¼‰
        self.font_commands = [
            r'\\mathbf', r'\\mathrm', r'\\mathit', 
            r'\\mathsf', r'\\mathtt', r'\\text', r'\\bm'
        ]
    
    def clean_latex(self, latex_str):
        """ARQMath ä¼˜åŒ–ç‰ˆæ¸…æ´—"""
        if not latex_str: 
            return "", False
        
        original = latex_str
        
        # 1. ç§»é™¤å®šç•Œç¬¦
        s = re.sub(r'\$\$?|\\\[|\\\]|\\\(|\\\)', '', latex_str)
        
        # 2. å‰¥ç¦»å­—ä½“è£…é¥°ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
        for cmd in self.font_commands:
            s = s.replace(cmd, '')
        
        # 3. ç¬¦å·åˆ«åæ›¿æ¢
        for old, new in self.sorted_symbols:
            s = s.replace(old, new)
        
        # 4. ç»Ÿä¸€çŸ©é˜µç¯å¢ƒ
        matrix_types = ['pmatrix', 'bmatrix', 'vmatrix', 'Vmatrix']
        for mtype in matrix_types:
            s = re.sub(rf'\\begin\{{{mtype}\}}', r'\\begin{matrix}', s)
            s = re.sub(rf'\\end\{{{mtype}\}}', r'\\end{matrix}', s)
        
        # 5. ç§»é™¤è§†è§‰è£…é¥°ï¼ˆä¿ç•™ \limitsï¼Œå½±å“è¯­ä¹‰ï¼‰
        s = re.sub(r'\\left|\\right|\\displaystyle', '', s)
        
        # 6. ç©ºæ ¼æ ‡å‡†åŒ–ï¼ˆé‡è¦ï¼šä¸è¦å®Œå…¨ç§»é™¤ï¼ï¼‰
        s = re.sub(r'\s+', ' ', s.strip())
        
        # 7. ç®€åŒ–å†—ä½™å¤§æ‹¬å·ï¼ˆä»…å•å­—ç¬¦ï¼Œä¿æŠ¤ä¸‹æ ‡ä¸Šæ ‡ï¼‰
        s = re.sub(r'\{([a-zA-Z0-9])\}', r'\1', s)
        
        # åˆ¤æ–­æ˜¯å¦å‘ç”Ÿå®è´¨æ€§æ”¹åŠ¨
        original_normalized = re.sub(r'\s+', ' ', 
                                     re.sub(r'\$\$?|\\\[|\\\]|\\\(|\\\)', '', original)).strip()
        is_normalized = (s != original_normalized)
        
        return s, is_normalized

# =========================== æŸ¥è¯¢å¤„ç† ===========================
def process_queries(base_path, hash_gen):
    """ä»å®˜æ–¹ TSV æå– Task 2 çš„æŸ¥è¯¢å…¬å¼"""
    tsv_path = base_path / "data" / "arqmath3" / "queries_arqmath3_task2.tsv"
    out_path = base_path / "data" / "processed" / "queries_full.json"
    out_path.parent.mkdir(exist_ok=True, parents=True)
    
    queries = {}
    queries_metadata = {}
    
    print(f"\nğŸ” æ­£åœ¨ä» TSV æå–æŸ¥è¯¢å…¬å¼...")
    if not tsv_path.exists():
        print(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ° {tsv_path}")
        return

    with open(tsv_path, 'r', encoding='utf-8') as f:
        reader = csv.reader(f, delimiter='\t')
        for row in reader:
            if len(row) >= 2:
                topic_id = row[0].strip()
                raw_latex = row[1].strip()
                
                # æ¸…æ´—å¹¶ç”Ÿæˆå“ˆå¸Œ
                clean_latex, is_norm = hash_gen.clean_latex(raw_latex)
                h_latex = hash_gen.generate_latex_hash(clean_latex)
                
                queries[topic_id] = clean_latex
                queries_metadata[topic_id] = {
                    "topic_id": topic_id,
                    "raw_latex": raw_latex,
                    "clean_latex": clean_latex,
                    "hash": h_latex,
                    "is_normalized": is_norm
                }

    # å¯¼å‡ºä¸¤ä¸ªæ–‡ä»¶ï¼šç®€åŒ–ç‰ˆå’Œè¯¦ç»†ç‰ˆ
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(queries, f, ensure_ascii=False, indent=2)
    
    with open(out_path.parent / "queries_metadata.json", 'w', encoding='utf-8') as f:
        json.dump(queries_metadata, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æŸ¥è¯¢é›†å·²å°±ç»ª: {len(queries)} æ¡")
    print(f"   - ç®€åŒ–ç‰ˆ -> {out_path}")
    print(f"   - è¯¦ç»†ç‰ˆ -> {out_path.parent}/queries_metadata.json")

# =========================== è¯­æ–™å¤„ç† (Visual ID å¯¹é½) ===========================
def process_corpus(num_shards=101):
    base_path = Path.cwd()
    latex_dir = base_path / "data" / "arqmath3" / "latex_representation_v3"
    
    hash_gen = ARQMathDualHashGenerator()
    h_index = Approach0HashIndex()
    
    # 1. å…ˆå¤„ç†æŸ¥è¯¢
    process_queries(base_path, hash_gen)
    
    # 2. æ ¸å¿ƒæ•°æ®ç»“æ„
    corpus = {}  # key: visual_id, value: å…¬å¼å…ƒæ•°æ®
    visual_id_stats = defaultdict(int)  # ç»Ÿè®¡æ¯ä¸ª visual_id å‡ºç°æ¬¡æ•°
    issue_stats = defaultdict(int)  # ç»Ÿè®¡ issue ç±»å‹åˆ†å¸ƒ
    
    latex_files = sorted(latex_dir.glob("*.tsv"))[:num_shards]
    print(f"\nğŸ”„ æ­£åœ¨å¤„ç† {len(latex_files)} ä¸ªè¯­æ–™åˆ†ç‰‡...")
    
    total_formulas = 0
    skipped_d = 0  # è·³è¿‡çš„ 'd' æ ‡è®°
    skipped_duplicate = 0  # è·³è¿‡çš„é‡å¤ visual_id
    
    for f in tqdm(latex_files, desc="Processing Shards"):
        with open(f, 'r', encoding='utf-8') as fin:
            reader = csv.reader(fin, delimiter='\t')
            next(reader, None)  # è·³è¿‡è¡¨å¤´
            
            for row in reader:
                if len(row) < 9: 
                    continue
                
                total_formulas += 1
                
                # README å­—æ®µç»“æ„
                formula_id = row[0].strip()
                post_id = row[1].strip()
                thread_id = row[2].strip()
                post_type = row[3].strip()
                comment_id = row[4].strip()
                old_visual_id = row[5].strip()
                visual_id = row[6].strip()
                issue = row[7].strip()
                raw_latex = row[8].strip()
                
                # ç»Ÿè®¡ issue åˆ†å¸ƒ
                if issue:
                    issue_stats[issue] += 1
                
                # è¿‡æ»¤è§„åˆ™ 1: è·³è¿‡ 'd' æ ‡è®°ï¼ˆä¸å­˜åœ¨äº XMLï¼‰
                if 'd' in issue:
                    skipped_d += 1
                    continue
                
                # è¿‡æ»¤è§„åˆ™ 2: Visual ID å»é‡ï¼ˆåŒä¸€å…¬å¼åªä¿ç•™ä¸€æ¬¡ï¼‰
                if visual_id in corpus:
                    skipped_duplicate += 1
                    visual_id_stats[visual_id] += 1
                    continue
                
                visual_id_stats[visual_id] = 1
                
                # æ¸…æ´—å…¬å¼
                clean_latex, is_norm = hash_gen.clean_latex(raw_latex)
                h_latex = hash_gen.generate_latex_hash(clean_latex)
                
                # å­˜å‚¨å…ƒæ•°æ®ï¼ˆkey å¿…é¡»æ˜¯ visual_idï¼ï¼‰
                corpus[visual_id] = {
                    "formula_id": formula_id,
                    "visual_id": visual_id,
                    "old_visual_id": old_visual_id,
                    "post_id": post_id,
                    "thread_id": thread_id,
                    "type": post_type,
                    "comment_id": comment_id,
                    "latex": raw_latex,
                    "latex_norm": clean_latex,
                    "hash": h_latex,
                    "is_normalized": is_norm,
                    "issue": issue
                }
                
                # æ„å»ºå“ˆå¸Œç´¢å¼•ï¼ˆå€’æ’ç´¢å¼•ï¼šhash -> [visual_ids]ï¼‰
                if h_latex not in h_index.index:
                    h_index.index[h_latex] = []
                h_index.index[h_latex].append(visual_id)

    # 3. ç»Ÿè®¡æŠ¥å‘Š
    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"   - æ€»å…¬å¼æ•°: {total_formulas:,}")
    print(f"   - å”¯ä¸€ Visual ID: {len(corpus):,}")
    print(f"   - è·³è¿‡ 'd' æ ‡è®°: {skipped_d:,}")
    print(f"   - è·³è¿‡é‡å¤ Visual ID: {skipped_duplicate:,}")
    print(f"   - å”¯ä¸€å“ˆå¸Œæ•°: {len(h_index.index):,}")
    print(f"\nğŸ“‹ Issue åˆ†å¸ƒ:")
    for issue_type, count in sorted(issue_stats.items()):
        print(f"   - '{issue_type}': {count:,}")
    
    # 4. å¯¼å‡ºæ•°æ®
    out_dir = base_path / "data" / "processed"
    out_dir.mkdir(exist_ok=True, parents=True)
    
    artifacts_dir = base_path / "artifacts"
    artifacts_dir.mkdir(exist_ok=True, parents=True)
    
    print("\nğŸ’¾ æ­£åœ¨å¯¼å‡ºå¯¹é½åçš„ç´¢å¼•æ•°æ®...")
    
    # å¯¼å‡ºè¯­æ–™å…ƒæ•°æ®
    with open(out_dir / "formulas.json", 'w', encoding='utf-8') as f:
        json.dump(corpus, f, ensure_ascii=False)
    
    # å¯¼å‡ºå“ˆå¸Œç´¢å¼•
    h_index.save(artifacts_dir / "approach0_index.pkl")
    
    # å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯
    stats = {
        "total_formulas": total_formulas,
        "unique_visual_ids": len(corpus),
        "skipped_d": skipped_d,
        "skipped_duplicate": skipped_duplicate,
        "unique_hashes": len(h_index.index),
        "issue_distribution": dict(issue_stats),
        "visual_id_collision_stats": {
            "max_collision": max(visual_id_stats.values()),
            "avg_collision": sum(visual_id_stats.values()) / len(visual_id_stats),
        }
    }
    
    with open(out_dir / "corpus_stats.json", 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å¤„ç†å®Œæˆï¼")
    print(f"   - è¯­æ–™å…ƒæ•°æ® -> {out_dir}/formulas.json")
    print(f"   - å“ˆå¸Œç´¢å¼• -> {artifacts_dir}/approach0_index.pkl")
    print(f"   - ç»Ÿè®¡ä¿¡æ¯ -> {out_dir}/corpus_stats.json")

# =========================== æ£€ç´¢æ¥å£ ===========================
def search_formula(query_latex, index_path, corpus_path, top_k=100):
    """
    ARQMath-3 å…¬å¼æ£€ç´¢æ¥å£
    
    Args:
        query_latex: æŸ¥è¯¢å…¬å¼çš„ LaTeX å­—ç¬¦ä¸²
        index_path: å“ˆå¸Œç´¢å¼•è·¯å¾„
        corpus_path: è¯­æ–™å…ƒæ•°æ®è·¯å¾„
        top_k: è¿”å› top-k ç»“æœ
    
    Returns:
        List of (visual_id, score, metadata)
    """
    # åŠ è½½ç´¢å¼•
    h_index = Approach0HashIndex()
    h_index.load(index_path)
    
    # åŠ è½½è¯­æ–™
    with open(corpus_path, 'r', encoding='utf-8') as f:
        corpus = json.load(f)
    
    # æ¸…æ´—æŸ¥è¯¢
    hash_gen = ARQMathDualHashGenerator()
    clean_query, _ = hash_gen.clean_latex(query_latex)
    h_query = hash_gen.generate_latex_hash(clean_query)
    
    # ç²¾ç¡®å“ˆå¸ŒåŒ¹é…
    visual_ids = h_index.search(h_query)
    
    # ç»„è£…ç»“æœ
    results = []
    for vid in visual_ids[:top_k]:
        if vid in corpus:
            results.append({
                "visual_id": vid,
                "score": 1.0,  # ç²¾ç¡®åŒ¹é…å¾—åˆ†
                "metadata": corpus[vid]
            })
    
    return results

def diagnose_index():
    """è¯Šæ–­ç´¢å¼•æ„å»ºæ˜¯å¦æ­£ç¡®"""
    base_path = Path.cwd()
    
    # åŠ è½½ç´¢å¼•å’Œè¯­æ–™
    h_index = Approach0HashIndex()
    h_index.load(base_path / "artifacts" / "approach0_index.pkl")
    
    with open(base_path / "data" / "processed" / "formulas.json", 'r') as f:
        corpus = json.load(f)
    
    with open(base_path / "data" / "processed" / "queries_metadata.json", 'r') as f:
        queries = json.load(f)
    
    print("\nğŸ” ç´¢å¼•è¯Šæ–­æŠ¥å‘Š")
    print("=" * 60)
    
    # 1. ç´¢å¼•åŸºæœ¬ä¿¡æ¯
    print(f"\n1ï¸âƒ£ ç´¢å¼•ç»Ÿè®¡:")
    print(f"   - å“ˆå¸Œæ¡¶æ•°é‡: {len(h_index.index):,}")
    print(f"   - è¯­æ–™ Visual ID æ•°é‡: {len(corpus):,}")
    
    # 2. éšæœºæŠ½æ ·æ£€æŸ¥ç´¢å¼•
    hash_gen = ARQMathDualHashGenerator()
    print(f"\n2ï¸âƒ£ éšæœºæŠ½æ ·éªŒè¯ (å‰ 5 ä¸ªå…¬å¼):")
    
    for i, (visual_id, metadata) in enumerate(list(corpus.items())[:5], 1):
        raw_latex = metadata['latex']
        clean_latex = metadata['latex_norm']
        stored_hash = metadata['hash']
        
        # é‡æ–°è®¡ç®—å“ˆå¸Œ
        recalc_clean, _ = hash_gen.clean_latex(raw_latex)
        recalc_hash = hash_gen.generate_latex_hash(recalc_clean)
        
        print(f"\n   æ ·æœ¬ {i}:")
        print(f"   - Visual ID: {visual_id}")
        print(f"   - åŸå§‹ LaTeX: {raw_latex[:60]}...")
        print(f"   - æ¸…æ´—å: {clean_latex[:60]}...")
        print(f"   - å­˜å‚¨å“ˆå¸Œ: {stored_hash[:16]}...")
        print(f"   - é‡ç®—å“ˆå¸Œ: {recalc_hash[:16]}...")
        print(f"   - å“ˆå¸Œä¸€è‡´: {'âœ…' if stored_hash == recalc_hash else 'âŒ'}")
        
        # æ£€æŸ¥ç´¢å¼•ä¸­æ˜¯å¦èƒ½æ‰¾åˆ°
        if recalc_hash in h_index.index:
            found_vids = h_index.index[recalc_hash]
            print(f"   - ç´¢å¼•æŸ¥æ‰¾: âœ… æ‰¾åˆ° {len(found_vids)} ä¸ªåŒ¹é…")
            print(f"   - æœ¬ Visual ID åœ¨ç»“æœä¸­: {'âœ…' if visual_id in found_vids else 'âŒ'}")
        else:
            print(f"   - ç´¢å¼•æŸ¥æ‰¾: âŒ æœªæ‰¾åˆ°")
    
    # 3. æŸ¥è¯¢é›†æ£€æŸ¥
    print(f"\n3ï¸âƒ£ æŸ¥è¯¢é›†éªŒè¯ (å‰ 3 ä¸ªæŸ¥è¯¢):")
    
    query_found = 0
    for i, (topic_id, query_meta) in enumerate(list(queries.items())[:3], 1):
        query_hash = query_meta['hash']
        query_latex = query_meta['raw_latex']
        
        print(f"\n   æŸ¥è¯¢ {i} (Topic {topic_id}):")
        print(f"   - åŸå§‹ LaTeX: {query_latex[:60]}...")
        print(f"   - æ¸…æ´—å: {query_meta['clean_latex'][:60]}...")
        print(f"   - æŸ¥è¯¢å“ˆå¸Œ: {query_hash[:16]}...")
        
        if query_hash in h_index.index:
            matches = h_index.index[query_hash]
            print(f"   - åŒ¹é…ç»“æœ: âœ… æ‰¾åˆ° {len(matches)} ä¸ª Visual ID")
            query_found += 1
            # æ˜¾ç¤ºå‰ 3 ä¸ªåŒ¹é…
            for vid in matches[:3]:
                if vid in corpus:
                    print(f"     - {vid}: {corpus[vid]['latex'][:50]}...")
        else:
            print(f"   - åŒ¹é…ç»“æœ: âŒ æœªæ‰¾åˆ°")
    
    print(f"\n4ï¸âƒ£ æŸ¥è¯¢è¦†ç›–ç‡:")
    print(f"   - å‰ 3 ä¸ªæŸ¥è¯¢ä¸­æœ‰åŒ¹é…: {query_found}/3")
    
    # 4. å“ˆå¸Œå†²çªåˆ†æ
    collision_counts = [len(vids) for vids in h_index.index.values()]
    print(f"\n5ï¸âƒ£ å“ˆå¸Œå†²çªç»Ÿè®¡:")
    print(f"   - å¹³å‡æ¯ä¸ªå“ˆå¸Œå¯¹åº” Visual ID æ•°: {sum(collision_counts) / len(collision_counts):.2f}")
    print(f"   - æœ€å¤§å†²çªæ•°: {max(collision_counts)}")
    print(f"   - å•ä¸€æ˜ å°„æ¯”ä¾‹: {sum(1 for c in collision_counts if c == 1) / len(collision_counts) * 100:.2f}%")
    
    # 5. éªŒè¯ Visual ID å”¯ä¸€æ€§
    all_vids_in_index = set()
    for vids in h_index.index.values():
        all_vids_in_index.update(vids)
    
    print(f"\n6ï¸âƒ£ Visual ID å®Œæ•´æ€§:")
    print(f"   - è¯­æ–™ä¸­çš„ Visual ID: {len(corpus):,}")
    print(f"   - ç´¢å¼•ä¸­çš„ Visual ID: {len(all_vids_in_index):,}")
    print(f"   - è¦†ç›–ç‡: {len(all_vids_in_index) / len(corpus) * 100:.2f}%")
    
    missing_vids = set(corpus.keys()) - all_vids_in_index
    if missing_vids:
        print(f"   - âš ï¸ æœ‰ {len(missing_vids)} ä¸ª Visual ID æœªåœ¨ç´¢å¼•ä¸­")
        print(f"   - ç¤ºä¾‹: {list(missing_vids)[:3]}")

def test_retrieval():
    """ä½¿ç”¨å®é™…å­˜åœ¨çš„æŸ¥è¯¢è¿›è¡Œæµ‹è¯•"""
    base_path = Path.cwd()
    
    # åŠ è½½æŸ¥è¯¢é›†
    with open(base_path / "data" / "processed" / "queries_metadata.json", 'r') as f:
        queries = json.load(f)
    
    print("\nğŸ§ª æµ‹è¯•æ£€ç´¢åŠŸèƒ½")
    print("=" * 60)
    
    # æµ‹è¯•å‰ 5 ä¸ªæŸ¥è¯¢
    test_count = 0
    found_count = 0
    
    for topic_id, query_meta in list(queries.items())[:5]:
        test_count += 1
        query_latex = query_meta['raw_latex']
        
        print(f"\næµ‹è¯• {test_count}: Topic {topic_id}")
        print(f"æŸ¥è¯¢å…¬å¼: {query_latex[:80]}...")
        
        results = search_formula(
            query_latex=query_latex,
            index_path=base_path / "artifacts" / "approach0_index.pkl",
            corpus_path=base_path / "data" / "processed" / "formulas.json",
            top_k=10
        )
        
        if results:
            found_count += 1
            print(f"âœ… æ‰¾åˆ° {len(results)} æ¡åŒ¹é…")
            
            # æ˜¾ç¤ºå‰ 3 ä¸ªç»“æœ
            for i, r in enumerate(results[:3], 1):
                print(f"   {i}. Visual ID: {r['visual_id']}")
                print(f"      LaTeX: {r['metadata']['latex'][:60]}...")
                print(f"      Post ID: {r['metadata']['post_id']}")
        else:
            print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…")
    
    print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"   - æµ‹è¯•æŸ¥è¯¢æ•°: {test_count}")
    print(f"   - æˆåŠŸæ‰¾åˆ°ç»“æœ: {found_count}")
    print(f"   - æˆåŠŸç‡: {found_count / test_count * 100:.1f}%")
    
    if found_count == 0:
        print(f"\nâš ï¸ è­¦å‘Š: æ‰€æœ‰æµ‹è¯•æŸ¥è¯¢éƒ½æœªæ‰¾åˆ°ç»“æœ")
        print(f"   å¯èƒ½çš„åŸå› :")
        print(f"   1. åªå¤„ç†äº† 10 ä¸ªåˆ†ç‰‡ï¼ŒæŸ¥è¯¢å…¬å¼å¯èƒ½åœ¨å…¶ä»–åˆ†ç‰‡ä¸­")
        print(f"   2. ç¬¦å·æ ‡å‡†åŒ–ç­–ç•¥å¯¼è‡´æŸ¥è¯¢å’Œè¯­æ–™çš„å“ˆå¸Œä¸åŒ¹é…")
        print(f"   3. æŸ¥è¯¢å…¬å¼åœ¨åŸå§‹æ•°æ®é›†ä¸­å°±ä¸å­˜åœ¨")



if __name__ == "__main__":
    import sys
    
    # æ­¥éª¤ 1: å¤„ç†è¯­æ–™ï¼ˆæµ‹è¯•ç”¨ 10 åˆ†ç‰‡ï¼‰
    print("æ­¥éª¤ 1: å¤„ç† ARQMath-3 è¯­æ–™")
    print("=" * 60)
    process_corpus(num_shards=10)
    
    # æ­¥éª¤ 2: è¯Šæ–­ç´¢å¼•
    print("\n\næ­¥éª¤ 2: è¯Šæ–­ç´¢å¼•æ„å»º")
    print("=" * 60)
    diagnose_index()
    
    # æ­¥éª¤ 3: æµ‹è¯•æ£€ç´¢
    print("\n\næ­¥éª¤ 3: æµ‹è¯•æ£€ç´¢åŠŸèƒ½")
    print("=" * 60)
    test_retrieval()
    
    # æ­¥éª¤ 4: å»ºè®®
    print("\n\næ­¥éª¤ 4: ä¸‹ä¸€æ­¥å»ºè®®")
    print("=" * 60)
    print("å¦‚æœæ£€ç´¢æµ‹è¯•æˆåŠŸç‡è¾ƒä½ï¼Œå¯èƒ½éœ€è¦:")
    print("1. å¤„ç†å®Œæ•´çš„ 101 ä¸ªåˆ†ç‰‡ä»¥è·å¾—å®Œæ•´è¦†ç›–")
    print("2. è°ƒæ•´ç¬¦å·æ ‡å‡†åŒ–ç­–ç•¥ï¼ˆARQMATH_SYMBOL_MAPPINGï¼‰")
    print("3. æ·»åŠ æ›´å¤šæ£€ç´¢çº§åˆ«ï¼ˆç»“æ„åŒ¹é…ã€è½¯åŒ¹é…ç­‰ï¼‰")
    print("\nè¿è¡Œå®Œæ•´ç‰ˆæœ¬:")
    print("   process_corpus(num_shards=101)")