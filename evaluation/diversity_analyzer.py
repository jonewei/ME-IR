import json
import csv
import numpy as np
from pathlib import Path
from tqdm import tqdm
from collections import Counter

# è·¯å¾„é…ç½®
FORMULAS_JSON = "data/processed/formulas.json"
QUERIES_JSON = "data/processed/queries_full.json"
LATEX_DIR = "data/arqmath3/latex_representation_v3"

def get_visual_id_frequencies():
    """ç»Ÿè®¡æ¯ä¸ª Visual ID åœ¨ 2826 ä¸‡åŸå§‹å®ä¾‹ä¸­å‡ºç°çš„é¢‘ç‡"""
    print("ğŸ“Š æ­£åœ¨ç»Ÿè®¡åŸå§‹è¯­æ–™ä¸­çš„ Visual ID é¢‘ç‡åˆ†å¸ƒ...")
    freq_map = Counter()
    tsv_files = sorted(list(Path(LATEX_DIR).glob("*.tsv")))
    
    for f in tqdm(tsv_files, desc="Scanning for frequencies"):
        with open(f, 'r', encoding='utf-8') as fin:
            reader = csv.reader(fin, delimiter='\t')
            next(reader, None)
            for row in reader:
                if len(row) > 6:
                    freq_map[row[6].strip()] += 1
    return freq_map

def analyze_diversity():
    # 1. å‡†å¤‡æ•°æ®
    freq_map = get_visual_id_frequencies()
    
    # åŠ è½½å·²æœ‰çš„è¯„æµ‹é€»è¾‘
    from evaluation.final_hybrid_evaluator import HybridEvaluator
    evaluator = HybridEvaluator()
    
    TOP_K = 100
    results_report = []

    print(f"\nğŸš€ å¼€å§‹å¯¹ Top-{TOP_K} ç»“æœè¿›è¡Œå¤šæ ·æ€§å»ºæ¨¡...")
    
    # éšæœºå– 20 æ¡æŸ¥è¯¢è¿›è¡Œæ·±åº¦åˆ†æ
    sample_queries = list(evaluator.queries.items())[:20]
    
    for qid, q_latex in tqdm(sample_queries, desc="Analyzing Queries"):
        # è·å–å»é‡åçš„çœŸå®æœç´¢ç»“æœ (Visual IDs)
        dedup_results = evaluator.search_single(q_latex)[:TOP_K]
        
        # è®¡ç®—â€œå†—ä½™å‹åŠ›â€ï¼šå¦‚æœæ²¡å»é‡ï¼Œè¿™äº›ç»“æœä¼šå æ®å¤šå°‘ç©ºé—´ï¼Ÿ
        # ä¾‹å¦‚ï¼šæ’åå‰ 10 çš„å…¬å¼å¦‚æœæ¯ä¸ªéƒ½é‡å¤äº† 5 æ¬¡ï¼Œé‚£å®ƒä»¬ä¼šæŒ¤å å‰ 50 ä¸ªæ’å
        total_slots_consumed = 0
        expanded_rank_at_10 = 0
        
        for i, vid in enumerate(dedup_results):
            freq = freq_map.get(vid, 1)
            total_slots_consumed += freq
            if i == 9: # è®°å½•å‰ 10 åè¢«æŒ¤å‹åˆ°äº†ä»€ä¹ˆä½ç½®
                expanded_rank_at_10 = total_slots_consumed

        # è®¡ç®—â€œæœ‰æ•ˆä¿¡æ¯å¢ç›Šâ€
        # åœ¨ 8.4M ç´¢å¼•ä¸­ï¼ŒTop-100 èƒ½ç»™ç”¨æˆ·å±•ç¤º 100 ç§ä¸åŒçš„æ•°å­¦æ€è·¯
        # åœ¨ 28M ç´¢å¼•ä¸­ï¼ŒTop-100 å¯èƒ½åªèƒ½å±•ç¤º 100/avg_freq ç§æ€è·¯
        diversity_gain = TOP_K / (total_slots_consumed / TOP_K)
        
        results_report.append({
            "qid": qid,
            "dedup_unique_count": len(dedup_results),
            "simulated_slots": total_slots_consumed,
            "rank_inflation": total_slots_consumed / TOP_K,
            "expanded_rank_at_10": expanded_rank_at_10
        })

    # 3. è¾“å‡ºå¤šæ ·æ€§æŠ¥å‘Š
    print("\n" + "="*60)
    print("ğŸ“ˆ æ£€ç´¢å¤šæ ·æ€§ä¸æ’åä¼˜åŒ–æŠ¥å‘Š")
    print("="*60)
    
    avg_inflation = np.mean([r['rank_inflation'] for r in results_report])
    avg_rank_10 = np.mean([r['expanded_rank_at_10'] for r in results_report])
    
    print(f"1. å¹³å‡æ’åé€šèƒ€ç‡ (Rank Inflation): {avg_inflation:.2f}x")
    print(f"   [è§£é‡Š]: å¦‚æœä¸å»é‡ï¼Œæœç´¢ç»“æœä¸­çš„å†—ä½™ä¼šä½¿åˆ—è¡¨é•¿åº¦è†¨èƒ€ {avg_inflation:.2f} å€ã€‚")
    print("-" * 60)
    print(f"2. å‰ 10 åçš„è§†è§‰æŒ¤å‹ (Top-10 Compression):")
    print(f"   [ç»“è®º]: å»é‡åçš„å‰ 10 ä¸ªå…¬å¼ï¼Œåœ¨åŸå§‹è¯­æ–™ä¸­å¹³å‡å æ®äº†å‰ {avg_rank_10:.1f} ä¸ªæ§½ä½ã€‚")
    print("-" * 60)
    print(f"3. æ ¸å¿ƒè´¡çŒ® (Key Contribution):")
    print(f"   å»é‡é€»è¾‘ä¸ºç”¨æˆ·åœ¨ Top-100 çª—å£å†…å¤šé‡Šæ”¾äº† {int(TOP_K * (avg_inflation-1))} ä¸ªæœ‰æ•ˆä¿¡æ¯æ§½ä½ã€‚")
    print("="*60)

if __name__ == "__main__":
    analyze_diversity()