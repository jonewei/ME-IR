# import json
# import numpy as np
# import pandas as pd
# from collections import defaultdict
# from scipy import stats
# from tabulate import tabulate
# import re # ç¡®ä¿æ–‡ä»¶å¼€å¤´å¯¼å…¥äº† re

# class Evaluator:
#     def __init__(self, qrel_path, sem_path, str_path):
#         self.qrel_path = qrel_path
#         self.sem_path = sem_path
#         self.str_path = str_path
#         self.load_data()

#     def load_data(self):
#         with open(self.qrel_path, 'r') as f: self.qrels = json.load(f)
#         with open(self.sem_path, 'r') as f: self.sem_run = json.load(f)
#         with open(self.str_path, 'r') as f: self.str_run = json.load(f)
#         with open("data/processed/queries_full.json", 'r') as f: self.queries = json.load(f)

#     def calculate_metrics(self, run_dict):
#         metrics = {"P@1": [], "MRR": [], "nDCG@10": [], "MAP": []}
#         for qid, target_docs in self.qrels.items():
#             if qid not in run_dict:
#                 for m in metrics: metrics[m].append(0)
#                 continue
            
#             # æŒ‰åˆ†æ•°æ’åºæ£€ç´¢ç»“æœ
#             retrieved = sorted(run_dict[qid].items(), key=lambda x: x[1], reverse=True)
#             relevant_docs = {str(k): v for k, v in target_docs.items() if v > 0}
            
#             if not relevant_docs: continue

#             # 1. P@1
#             metrics["P@1"].append(1 if retrieved[0][0] in relevant_docs else 0)

#             # 2. MRR
#             mrr = 0
#             for i, (doc_id, _) in enumerate(retrieved):
#                 if str(doc_id) in relevant_docs:
#                     mrr = 1.0 / (i + 1)
#                     break
#             metrics["MRR"].append(mrr)

#             # 3. nDCG@10
#             dcg = 0
#             for i, (doc_id, _) in enumerate(retrieved[:10]):
#                 if str(doc_id) in relevant_docs:
#                     rel = relevant_docs[str(doc_id)]
#                     dcg += rel / np.log2(i + 2)
            
#             # è®¡ç®— IDCG (ç†æƒ³æƒ…å†µä¸‹çš„æœ€é«˜å¾—åˆ†)
#             rel_scores = sorted(relevant_docs.values(), reverse=True)
#             idcg = sum([rel / np.log2(i + 2) for i, rel in enumerate(rel_scores[:10])])
#             metrics["nDCG@10"].append(dcg / idcg if idcg > 0 else 0)

#             # 4. MAP
#             ap, hits = 0, 0
#             for i, (doc_id, _) in enumerate(retrieved):
#                 if str(doc_id) in relevant_docs:
#                     hits += 1
#                     ap += hits / (i + 1)
#             metrics["MAP"].append(ap / len(relevant_docs) if relevant_docs else 0)

#         return {k: np.mean(v) for k, v in metrics.items()}, metrics["MRR"]

#     def reciprocal_rank_fusion(self, w_sem=1.0, w_str=0.3, k_rrf=60):
#         """åŠ æƒ RRFï¼šé€šè¿‡é™ä½ç»“æ„æµæƒé‡å‡å°‘å™ªå£°"""
#         fused_run = defaultdict(dict)
#         all_qids = set(self.sem_run.keys()) | set(self.str_run.keys())
        
#         for qid in all_qids:
#             scores = defaultdict(float)
#             if qid in self.sem_run:
#                 sorted_sem = sorted(self.sem_run[qid].items(), key=lambda x: x[1], reverse=True)
#                 for rank, (doc_id, _) in enumerate(sorted_sem):
#                     scores[doc_id] += w_sem / (k_rrf + rank + 1)
#             if qid in self.str_run:
#                 sorted_str = sorted(self.str_run[qid].items(), key=lambda x: x[1], reverse=True)
#                 for rank, (doc_id, _) in enumerate(sorted_str):
#                     scores[doc_id] += w_str / (k_rrf + rank + 1)
#             fused_run[qid] = dict(scores)
#         return fused_run

#     # def run_ablation(self):
#     #     print("\n>>> æ‰§è¡ŒåŠ æƒæ¶ˆèå®éªŒ (Weighted Ablation Study)...")
#     #     results = []
        
#     #     # S1: Semantic
#     #     m_s1, mrr_s1 = self.calculate_metrics(self.sem_run)
#     #     results.append({"Setting": "S1: Semantic only", **m_s1})
        
#     #     # S2: Structural
#     #     m_s2, _ = self.calculate_metrics(self.str_run)
#     #     results.append({"Setting": "S2: Structural only", **m_s2})
        
#     #     # S4: LS-MIR (Weighted Fusion)
#     #     fused = self.reciprocal_rank_fusion(w_sem=1.0, w_str=0.9)
#     #     m_s4, mrr_s4 = self.calculate_metrics(fused)
#     #     results.append({"Setting": "S4: LS-MIR (Proposed)", **m_s4})
        
#     #     print(tabulate(pd.DataFrame(results), headers='keys', tablefmt='pipe', floatfmt=".4f"))
        
#     #     t_stat, p_val = stats.ttest_rel(mrr_s1, mrr_s4)
#     #     print(f"\n[Statistical Significance] p-value: {p_val:.6e}")

#     # import re # ç¡®ä¿ä½ åœ¨æ–‡ä»¶æœ€ä¸Šæ–¹æ·»åŠ äº†è¿™ä¸€è¡Œ
#     def run_dynamic_optimization(self):
#         print("\n>>> æ­£åœ¨å¼€å¯åŠ¨æ€æƒé‡æœç´¢ (Dynamic Weight Optimization)...")
        
#         # 1. è®¡ç®—åŸºå‡† (S1: Semantic Only) çš„ MRR åºåˆ—ç”¨äºæ˜¾è‘—æ€§æ ¡éªŒ
#         m_s1, mrr_s1_list = self.calculate_metrics(self.sem_run)
        
#         search_results = []
#         best_mrr = -1
#         best_w = 0
        
#         # 2. éå†æƒé‡ç©ºé—´
#         weights = np.arange(0.1, 1.1, 0.1) # 0.1, 0.2, ..., 1.0
#         for w in weights:
#             # æ‰§è¡Œæ··åˆæ£€ç´¢
#             fused = self.reciprocal_rank_fusion(w_sem=1.0, w_str=w)
#             # è®¡ç®—å„é¡¹æŒ‡æ ‡
#             metrics, mrr_list = self.calculate_metrics(fused)
            
#             # è®¡ç®—æ˜¾è‘—æ€§ (å¯¹æ¯” S1)
#             t_stat, p_val = stats.ttest_rel(mrr_s1_list, mrr_list)
            
#             res = {
#                 "w_str": w,
#                 "P@1": metrics["P@1"],
#                 "MRR": metrics["MRR"],
#                 "p-value": p_val,
#                 "Significant": "YES" if p_val < 0.05 else "NO"
#             }
#             search_results.append(res)
            
#             # è®°å½•æœ€ä¼˜ MRR
#             if metrics["MRR"] > best_mrr:
#                 best_mrr = metrics["MRR"]
#                 best_w = w

#         # 3. è¾“å‡ºæœç´¢ç»“æœè¡¨æ ¼
#         df_res = pd.DataFrame(search_results)
#         print(tabulate(df_res, headers='keys', tablefmt='pipe', floatfmt=".4f"))
        
#         print(f"\nâœ… æœç´¢å®Œæˆï¼åœ¨ w_str = {best_w:.1f} æ—¶å–å¾—æœ€ä¼˜ MRR: {best_mrr:.4f}")
#         return best_w

#     def run_dynamic_optimization(self):
#         print("\n>>> æ­£åœ¨å¼€å¯åŠ¨æ€æƒé‡æœç´¢ (Dynamic Weight Optimization)...")
        
#         # 1. è®¡ç®—åŸºå‡† (S1: Semantic Only) çš„ MRR åºåˆ—ç”¨äºæ˜¾è‘—æ€§æ ¡éªŒ
#         m_s1, mrr_s1_list = self.calculate_metrics(self.sem_run)
        
#         search_results = []
#         best_mrr = -1
#         best_w = 0
        
#         # 2. éå†æƒé‡ç©ºé—´
#         weights = np.arange(0.1, 1.1, 0.1) # 0.1, 0.2, ..., 1.0
#         for w in weights:
#             # æ‰§è¡Œæ··åˆæ£€ç´¢
#             fused = self.reciprocal_rank_fusion(w_sem=1.0, w_str=w)
#             # è®¡ç®—å„é¡¹æŒ‡æ ‡
#             metrics, mrr_list = self.calculate_metrics(fused)
            
#             # è®¡ç®—æ˜¾è‘—æ€§ (å¯¹æ¯” S1)
#             t_stat, p_val = stats.ttest_rel(mrr_s1_list, mrr_list)
            
#             res = {
#                 "w_str": w,
#                 "P@1": metrics["P@1"],
#                 "MRR": metrics["MRR"],
#                 "p-value": p_val,
#                 "Significant": "YES" if p_val < 0.05 else "NO"
#             }
#             search_results.append(res)
            
#             # è®°å½•æœ€ä¼˜ MRR
#             if metrics["MRR"] > best_mrr:
#                 best_mrr = metrics["MRR"]
#                 best_w = w

#         # 3. è¾“å‡ºæœç´¢ç»“æœè¡¨æ ¼
#         df_res = pd.DataFrame(search_results)
#         print(tabulate(df_res, headers='keys', tablefmt='pipe', floatfmt=".4f"))
        
#         print(f"\nâœ… æœç´¢å®Œæˆï¼åœ¨ w_str = {best_w:.1f} æ—¶å–å¾—æœ€ä¼˜ MRR: {best_mrr:.4f}")
#         return best_w

#     def run_complexity_analysis(self):
#         print("\n>>> æ‰§è¡Œå¤æ‚åº¦æ·±åº¦åˆ†æ (Token-based)...")
#         # ä¿æŒä¸æ¶ˆèå®éªŒä¸€è‡´çš„æƒé‡
#         fused = self.reciprocal_rank_fusion(w_sem=1.0, w_str=0.9)
#         complexity_res = []
        
#         # å®šä¹‰å¤æ‚åº¦åŒºé—´ (Token æ•°é‡)
#         categories = {
#             "Simple (<20)": (0, 20), 
#             "Medium (20-50)": (20, 50), 
#             "Complex (>50)": (50, 9999)
#         }
        
#         # å¤‡ä»½åŸå§‹çœŸå€¼è¡¨
#         original_qrels = self.qrels
        
#         for name, (low, high) in categories.items():
#             cat_qids = []
#             for qid, text in self.queries.items():
#                 if qid not in original_qrels:
#                     continue
                
#                 # --- æ ¸å¿ƒæ”¹è¿›ï¼šä½¿ç”¨æ­£åˆ™ç»Ÿè®¡ LaTeX Token æ•° ---
#                 # ç»Ÿè®¡åæ–œæ å‘½ä»¤ (\int), å•è¯ (x), ä»¥åŠç‰¹æ®Šç®—å­ ({}, ^, _, +, =)
#                 tokens = re.findall(r'\\[a-zA-Z]+|[\w]+|[{}()^|_=+]', str(text))
#                 token_count = len(tokens)
                
#                 if low <= token_count < high:
#                     cat_qids.append(qid)
            
#             if not cat_qids:
#                 continue
                
#             # æå–å½“å‰ç±»åˆ«çš„çœŸå€¼å’Œæ£€ç´¢ç»“æœ
#             self.qrels = {qid: original_qrels[qid] for qid in cat_qids}
#             cat_run = {qid: fused[qid] for qid in cat_qids if qid in fused}
            
#             # è®¡ç®—è¯¥ç±»åˆ«çš„æŒ‡æ ‡
#             m, _ = self.calculate_metrics(cat_run)
            
#             complexity_res.append({
#                 "Category": name, 
#                 "Count": len(cat_qids), 
#                 "MRR": m["MRR"],
#                 "P@1": m["P@1"]
#             })
        
#         # è¿˜åŸçœŸå€¼è¡¨
#         self.qrels = original_qrels
        
#         # è¾“å‡ºè¡¨æ ¼
#         print(tabulate(pd.DataFrame(complexity_res), headers='keys', tablefmt='pipe', floatfmt=".4f"))

# if __name__ == "__main__":
#     evaluator = Evaluator(
#         qrel_path="data/qrel_76_expert.json",
#         sem_path="results/raw_sem_scores.json",
#         str_path="results/raw_str_scores.json"
#     )
#     evaluator.run_ablation()
#     evaluator.run_complexity_analysis()


import json
import numpy as np
import pandas as pd
from collections import defaultdict
from scipy import stats
from tabulate import tabulate
import re
import time # ç¡®ä¿åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥äº† time

class Evaluator:
    def __init__(self, qrel_path, sem_path, str_path, query_path):
        self.qrel_path = qrel_path
        self.sem_path = sem_path
        self.str_path = str_path
        self.query_path = query_path
        self.load_data()

    def load_data(self):
        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®æº...")
        with open(self.qrel_path, 'r') as f: self.qrels = json.load(f)
        with open(self.sem_path, 'r') as f: self.sem_run = json.load(f)
        with open(self.str_path, 'r') as f: self.str_run = json.load(f)
        with open(self.query_path, 'r') as f: self.queries = json.load(f)
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆã€‚æœ‰æ•ˆæŸ¥è¯¢æ•°: {len(self.qrels)}")


    def run_latency_audit(self, best_w):
        """æµ‹é‡èåˆç®—æ³•çš„å·¥ç¨‹æ•ˆç‡ (é’ˆå¯¹ 76 ä¸ªæŸ¥è¯¢)"""
        print("\n>>> æ‰§è¡Œæ£€ç´¢æ•ˆç‡å®¡è®¡ (Latency Audit)...")
        
        start_time = time.time()
        # æ¨¡æ‹Ÿæ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„èåˆè¿‡ç¨‹
        for _ in range(10): # è¿è¡Œ 10 æ¬¡å–å¹³å‡ä»¥æ¶ˆé™¤ç³»ç»Ÿæ‰°åŠ¨
            _ = self.reciprocal_rank_fusion(w_sem=1.0, w_str=best_w)
        
        end_time = time.time()
        
        # è®¡ç®—å•æ¬¡èåˆçš„æ€»è€—æ—¶ (é’ˆå¯¹ 76 ä¸ªæŸ¥è¯¢)
        total_fusion_avg = ((end_time - start_time) / 10) * 1000 # æ¯«ç§’
        # è®¡ç®—å•ä¸ªæŸ¥è¯¢çš„å¹³å‡èåˆè€—æ—¶
        per_query_fusion = total_fusion_avg / len(self.qrels)
        
        print(f"| ç»Ÿè®¡é¡¹ | æ•°å€¼ |")
        print(f"| :--- | :--- |")
        print(f"| è¯„ä¼°æŸ¥è¯¢æ€»æ•° | {len(self.qrels)} |")
        print(f"| å•ä¸ªæŸ¥è¯¢å¹³å‡èåˆè€—æ—¶ (RRF) | {per_query_fusion:.2f} ms |")
        print(f"\nğŸ’¡ æç¤ºï¼šRRF é˜¶æ®µè€—æ—¶æä½ï¼Œç³»ç»Ÿæ€»å»¶è¿Ÿçš„ä¸»è¦æ¥æºæ˜¯ç´¢å¼•æ£€ç´¢é˜¶æ®µã€‚")

    def calculate_metrics(self, run_dict):
        """è®¡ç®—æ ¸å¿ƒ IR æŒ‡æ ‡"""
        metrics = {"P@1": [], "MRR": [], "nDCG@10": [], "MAP": []}
        
        for qid, target_docs in self.qrels.items():
            if qid not in run_dict or not run_dict[qid]:
                for m in metrics: metrics[m].append(0)
                continue
            
            # æŒ‰åˆ†æ•°ä»é«˜åˆ°ä½æ’åºç»“æœ
            retrieved = sorted(run_dict[qid].items(), key=lambda x: x[1], reverse=True)
            relevant_docs = {str(k): v for k, v in target_docs.items() if v > 0}
            
            if not relevant_docs: continue

            # 1. P@1
            metrics["P@1"].append(1 if str(retrieved[0][0]) in relevant_docs else 0)

            # 2. MRR
            mrr = 0
            for i, (doc_id, _) in enumerate(retrieved):
                if str(doc_id) in relevant_docs:
                    mrr = 1.0 / (i + 1)
                    break
            metrics["MRR"].append(mrr)

            # 3. nDCG@10
            dcg = 0
            for i, (doc_id, _) in enumerate(retrieved[:10]):
                if str(doc_id) in relevant_docs:
                    rel = relevant_docs[str(doc_id)]
                    dcg += rel / np.log2(i + 2)
            
            rel_scores = sorted(relevant_docs.values(), reverse=True)
            idcg = sum([rel / np.log2(i + 2) for i, rel in enumerate(rel_scores[:10])])
            metrics["nDCG@10"].append(dcg / idcg if idcg > 0 else 0)

            # 4. MAP
            ap, hits = 0, 0
            for i, (doc_id, _) in enumerate(retrieved):
                if str(doc_id) in relevant_docs:
                    hits += 1
                    ap += hits / (i + 1)
            metrics["MAP"].append(ap / len(relevant_docs) if relevant_docs else 0)

        return {k: np.mean(v) for k, v in metrics.items()}, metrics["MRR"]

    def reciprocal_rank_fusion(self, w_sem=1.0, w_str=0.3, k_rrf=60):
        """åŠ æƒ RRF èåˆé€»è¾‘"""
        fused_run = defaultdict(dict)
        all_qids = set(self.sem_run.keys()) | set(self.str_run.keys())
        
        for qid in all_qids:
            scores = defaultdict(float)
            # å¤„ç†è¯­ä¹‰æµ
            if qid in self.sem_run:
                sorted_sem = sorted(self.sem_run[qid].items(), key=lambda x: x[1], reverse=True)
                for rank, (doc_id, _) in enumerate(sorted_sem[:1000]):
                    scores[doc_id] += w_sem / (k_rrf + rank + 1)
            # å¤„ç†ç»“æ„æµ
            if qid in self.str_run:
                sorted_str = sorted(self.str_run[qid].items(), key=lambda x: x[1], reverse=True)
                for rank, (doc_id, _) in enumerate(sorted_str[:1000]):
                    scores[doc_id] += w_str / (k_rrf + rank + 1)
            fused_run[qid] = dict(scores)
        return fused_run

    def run_dynamic_optimization(self):
        """åŠ¨æ€è¶…å‚æ•°æœç´¢ï¼šå¯»æ‰¾æ€§èƒ½ä¸æ˜¾è‘—æ€§çš„å¹³è¡¡ç‚¹"""
        print("\n>>> æ­£åœ¨å¼€å¯åŠ¨æ€æƒé‡æœç´¢ (Grid Search for w_str)...")
        m_s1, mrr_s1_list = self.calculate_metrics(self.sem_run)
        
        search_results = []
        best_mrr = -1
        optimal_w = 0
        
        weights = np.arange(0.1, 1.1, 0.1)
        for w in weights:
            fused = self.reciprocal_rank_fusion(w_sem=1.0, w_str=w)
            metrics, mrr_list = self.calculate_metrics(fused)
            _, p_val = stats.ttest_rel(mrr_s1_list, mrr_list)
            
            res = {
                "w_str": round(w, 1),
                "P@1": metrics["P@1"],
                "MRR": metrics["MRR"],
                "nDCG@10": metrics["nDCG@10"],
                "p-value": p_val,
                "Sig (<0.05)": "âœ…" if p_val < 0.05 else "âŒ"
            }
            search_results.append(res)
            
            if metrics["MRR"] > best_mrr:
                best_mrr = metrics["MRR"]
                optimal_w = w

        print(tabulate(pd.DataFrame(search_results), headers='keys', tablefmt='pipe', floatfmt=".4f"))
        print(f"\nğŸ’¡ å»ºè®®æœ€ä¼˜æƒé‡: w_str = {optimal_w:.1f} (MRR: {best_mrr:.4f})")
        return optimal_w

    def run_complexity_analysis(self, best_w):
        """åŸºäº Token é•¿åº¦çš„å¤æ‚åº¦æ·±åº¦åˆ†æ"""
        print(f"\n>>> æ‰§è¡Œå¤æ‚åº¦æ·±åº¦åˆ†æ (Token-based, w_str={best_w:.1f})...")
        fused = self.reciprocal_rank_fusion(w_sem=1.0, w_str=best_w)
        
        categories = {
            "Simple (<20)": (0, 20), 
            "Medium (20-50)": (20, 50), 
            "Complex (>50)": (50, 999)
        }
        
        original_qrels = self.qrels
        complexity_res = []
        
        for name, (low, high) in categories.items():
            cat_qids = []
            for qid, query_obj in self.queries.items():
                # å…¼å®¹ä¸åŒ JSON æ ¼å¼
                text = query_obj['latex'] if isinstance(query_obj, dict) else str(query_obj)
                if qid not in original_qrels: continue
                
                # Token ç»Ÿè®¡æ­£åˆ™
                tokens = re.findall(r'\\[a-zA-Z]+|[\w]+|[{}()^|_=+]', text)
                if low <= len(tokens) < high:
                    cat_qids.append(qid)
            
            if not cat_qids: continue
            
            self.qrels = {qid: original_qrels[qid] for qid in cat_qids}
            cat_run = {qid: fused[qid] for qid in cat_qids}
            m, _ = self.calculate_metrics(cat_run)
            
            complexity_res.append({
                "Category": name, "Count": len(cat_qids), 
                "MRR": m["MRR"], "P@1": m["P@1"]
            })
        
        self.qrels = original_qrels # è¿˜åŸ
        print(tabulate(pd.DataFrame(complexity_res), headers='keys', tablefmt='pipe', floatfmt=".4f"))

if __name__ == "__main__":
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = Evaluator(
        qrel_path="data/qrel_76_expert.json",
        sem_path="results/raw_sem_scores.json",
        str_path="results/raw_str_scores.json",
        query_path="data/processed/queries_full.json"
    )
    
    # 1. è¿è¡ŒåŠ¨æ€æœç´¢ï¼Œæ‰¾åˆ°æœ€ä½³æƒé‡
    best_w = evaluator.run_dynamic_optimization()
    
    # 2. åŸºäºæœ€ä½³æƒé‡è¿è¡Œå¤æ‚åº¦åˆ†æ
    evaluator.run_complexity_analysis(best_w)
    # 3. æ–°å¢ï¼šè¿è¡Œæ•ˆç‡å®¡è®¡
    evaluator.run_latency_audit(best_w)

