import json
import torch
import faiss
import numpy as np
import re
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from pathlib import Path
from collections import defaultdict

# ==================== é…ç½®å‚æ•° (å¿…é¡»ä¸æ„å»ºè„šæœ¬ä¸€è‡´) ====================
MODEL_NAME = 'math-similarity/Bert-MLM_arXiv-MP-class_zbMath'
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
INDEX_PATH = "artifacts/vector_index_full_v3.faiss"
MAPPING_PATH = "artifacts/vector_id_mapping_v3.json"
LABEL_PATH = "data/processed/relevance_labels.json"
QUERY_PATH = "data/processed/queries_full.json"
TOP_K = 1000

# =========================== ğŸ”§ ç»Ÿä¸€çš„LaTeXæ¸…æ´—å‡½æ•° ===========================
def clean_latex(latex_str):
    """
    âš ï¸  å¿…é¡»ä¸prepare_final_arqmath.pyå’Œbuild_full_4090_v3.pyå®Œå…¨ä¸€è‡´ï¼
    """
    if not latex_str: 
        return ""
    
    # ç§»é™¤æ•°å­¦æ¨¡å¼æ ‡è®°
    latex_str = re.sub(r'\$\$?|\\\[|\\\]', '', latex_str)
    
    # æ ‡å‡†åŒ–å‘½ä»¤
    latex_str = re.sub(r'\\dfrac|\\tfrac', r'\\frac', latex_str)
    latex_str = re.sub(r'\\left|\\right', '', latex_str)
    
    # å‹ç¼©ç©ºæ ¼
    latex_str = re.sub(r'\s+', ' ', latex_str.strip())
    
    # å°å†™åŒ–
    latex_str = latex_str.lower()
    
    return latex_str

# =========================== è¯„æµ‹å¼•æ“ ===========================
class MathEvaluator:
    def __init__(self):
        print(f"ğŸ“¦ æ­£åœ¨åŠ è½½è¯„æµ‹ç¯å¢ƒ...")
        
        # åŠ è½½æ¨¡å‹
        print(f"   - æ¨¡å‹: {MODEL_NAME}")
        self.model = SentenceTransformer(MODEL_NAME, device=DEVICE)
        
        # åŠ è½½ç´¢å¼•
        print(f"   - ç´¢å¼•: {INDEX_PATH}")
        self.index = faiss.read_index(INDEX_PATH)
        
        # åŠ è½½IDæ˜ å°„
        with open(MAPPING_PATH, 'r') as f:
            self.fids = json.load(f)
        
        # åŠ è½½æ ‡å‡†ç­”æ¡ˆ
        with open(LABEL_PATH, 'r') as f:
            self.relevance = json.load(f)
        
        # åŠ è½½æŸ¥è¯¢
        with open(QUERY_PATH, 'r') as f:
            queries_raw = json.load(f)
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ­£ç¡®è§£æqueriesæ•°æ®ç»“æ„
        self.queries = {}
        for qid, qdata in queries_raw.items():
            # æ£€æŸ¥æ•°æ®ç»“æ„
            if isinstance(qdata, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼Œæå–latexæˆ–latex_norm
                latex = qdata.get('latex_norm') or qdata.get('latex', '')
            elif isinstance(qdata, str):
                # å¦‚æœç›´æ¥æ˜¯å­—ç¬¦ä¸²
                latex = qdata
            else:
                print(f"   âš ï¸  è­¦å‘Š: æŸ¥è¯¢ {qid} çš„æ•°æ®æ ¼å¼å¼‚å¸¸")
                continue
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¯¹æŸ¥è¯¢ä¹Ÿè¿›è¡ŒåŒæ ·çš„æ¸…æ´—
            self.queries[qid] = clean_latex(latex)
        
        print(f"   âœ… åŠ è½½å®Œæˆ")
        print(f"      - ç´¢å¼•å‘é‡æ•°: {self.index.ntotal:,}")
        print(f"      - æŸ¥è¯¢æ•°: {len(self.queries)}")
        print(f"      - æ ‡å‡†ç­”æ¡ˆæ•°: {len(self.relevance)}")

    def run_evaluation(self, save_results=True):
        """æ‰§è¡Œå®Œæ•´è¯„æµ‹"""
        
        # 1. å‡†å¤‡æŸ¥è¯¢æ•°æ®
        topic_ids = []
        query_texts = []
        
        for tid, latex in self.queries.items():
            topic_ids.append(tid)
            query_texts.append(latex)
        
        print(f"\nğŸ” æ­£åœ¨ç¼–ç  {len(topic_ids)} æ¡æŸ¥è¯¢å…¬å¼...")
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        print(f"\nğŸ“Š æŸ¥è¯¢æ•°æ®è´¨é‡æ£€æŸ¥:")
        for i in range(min(3, len(query_texts))):
            print(f"   [{topic_ids[i]}]: {query_texts[i][:60]}...")
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿normalize_embeddings=True
        query_embs = self.model.encode(
            query_texts, 
            batch_size=32, 
            normalize_embeddings=True,  # å¿…é¡»ä¸ç´¢å¼•ç«¯ä¸€è‡´
            show_progress_bar=True,
            convert_to_numpy=True
        ).astype('float32')

        print(f"\nâš¡ æ­£åœ¨æ£€ç´¢ Top-{TOP_K}...")
        distances, indices = self.index.search(query_embs, TOP_K)

        # 2. è®¡ç®—RecallæŒ‡æ ‡
        recall_scores = []
        precision_scores = []
        query_details = []
        
        print(f"\nğŸ“Š æ­£åœ¨è®¡ç®—è¯„æµ‹æŒ‡æ ‡...")
        for i, topic_id in enumerate(tqdm(topic_ids, desc="Processing")):
            # è·å–æ ‡å‡†ç­”æ¡ˆé›†åˆ
            gt_docs = set(self.relevance.get(topic_id, {}).keys())
            if not gt_docs:
                continue
            
            # è·å–æ£€ç´¢ç»“æœ
            retrieved_indices = indices[i]
            retrieved_fids = [str(self.fids[idx]) for idx in retrieved_indices if idx != -1]
            
            # ç»Ÿä¸€IDæ ¼å¼ï¼ˆå¤„ç†å¯èƒ½çš„int/strä¸ä¸€è‡´ï¼‰
            retrieved_set = set(retrieved_fids)
            gt_set = set(str(x) for x in gt_docs)
            
            # è®¡ç®—æŒ‡æ ‡
            hits = len(gt_set.intersection(retrieved_set))
            recall = hits / len(gt_set) if len(gt_set) > 0 else 0
            precision = hits / len(retrieved_set) if len(retrieved_set) > 0 else 0
            
            recall_scores.append(recall)
            precision_scores.append(precision)
            
            # ä¿å­˜è¯¦ç»†ä¿¡æ¯ç”¨äºé”™è¯¯åˆ†æ
            query_details.append({
                'topic_id': topic_id,
                'query': query_texts[i][:100],
                'gt_count': len(gt_set),
                'retrieved_count': len(retrieved_set),
                'hits': hits,
                'recall': recall,
                'precision': precision,
                'top5_distances': distances[i][:5].tolist(),
                'top5_ids': retrieved_fids[:5]
            })

        # 3. è¾“å‡ºç»“æœ
        avg_recall = np.mean(recall_scores) * 100
        avg_precision = np.mean(precision_scores) * 100
        
        print("\n" + "="*60)
        print(f"ğŸ† å‘é‡æ£€ç´¢è¯„æµ‹ç»“æœ (Stage 2 Only)")
        print("="*60)
        print(f"æ¨¡å‹: {MODEL_NAME}")
        print(f"ç´¢å¼•è§„æ¨¡: {self.index.ntotal:,} æ¡å…¬å¼")
        print(f"æŸ¥è¯¢æ•°é‡: {len(recall_scores)}")
        print(f"-" * 60)
        print(f"Mean Recall@{TOP_K}:    {avg_recall:.2f}%")
        print(f"Mean Precision@{TOP_K}: {avg_precision:.2f}%")
        print("="*60)
        
        # 4. é”™è¯¯åˆ†æ
        print(f"\nğŸ“ˆ å¬å›ç‡åˆ†å¸ƒ:")
        bins = [0, 0.01, 0.05, 0.1, 0.2, 0.5, 1.0]
        bin_names = ["0%", "0-1%", "1-5%", "5-10%", "10-20%", "20-50%", "50-100%"]
        for i in range(len(bins)-1):
            count = sum(1 for r in recall_scores if bins[i] <= r < bins[i+1])
            pct = count / len(recall_scores) * 100
            print(f"   {bin_names[i+1]}: {count:3d} queries ({pct:5.1f}%)")
        
        # 5. å±•ç¤ºæœ€å¥½å’Œæœ€å·®çš„æ¡ˆä¾‹
        sorted_details = sorted(query_details, key=lambda x: x['recall'], reverse=True)
        
        print(f"\nâœ… Top 3 æœ€ä½³å¬å›æ¡ˆä¾‹:")
        for detail in sorted_details[:3]:
            print(f"\n   Topic: {detail['topic_id']}")
            print(f"   Query: {detail['query']}")
            print(f"   Recall: {detail['recall']*100:.1f}% ({detail['hits']}/{detail['gt_count']})")
            print(f"   Top-1 è·ç¦»: {detail['top5_distances'][0]:.4f}")
        
        print(f"\nâŒ Top 3 æœ€å·®å¬å›æ¡ˆä¾‹:")
        for detail in sorted_details[-3:]:
            print(f"\n   Topic: {detail['topic_id']}")
            print(f"   Query: {detail['query']}")
            print(f"   Recall: {detail['recall']*100:.1f}% ({detail['hits']}/{detail['gt_count']})")
            print(f"   Top-1 è·ç¦»: {detail['top5_distances'][0]:.4f}")
            print(f"   Top-5 IDs: {detail['top5_ids']}")
        
        # 6. ä¿å­˜è¯¦ç»†ç»“æœ
        if save_results:
            results_path = Path("evaluation_results")
            results_path.mkdir(exist_ok=True)
            
            with open(results_path / "vector_recall_details.json", 'w') as f:
                json.dump({
                    'summary': {
                        'mean_recall': avg_recall,
                        'mean_precision': avg_precision,
                        'num_queries': len(recall_scores),
                        'index_size': self.index.ntotal
                    },
                    'details': query_details
                }, f, indent=2)
            
            print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {results_path / 'vector_recall_details.json'}")
        
        return avg_recall

if __name__ == "__main__":
    evaluator = MathEvaluator()
    evaluator.run_evaluation()