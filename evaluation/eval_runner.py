
"""
Enhanced evaluation runner with TREC qrel support and corrected nDCG calculation.

Key fixes:
1. âœ… Support for TSV qrel format (B.301 0 60069 3)
2. âœ… Corrected nDCG@K calculation (fixed IDCG truncation)
3. âœ… Robust error handling
4. âœ… Progress bar integration
"""

import numpy as np
from tqdm import tqdm
import json
import logging

logger = logging.getLogger(__name__)


def load_qrel_labels(qrel_path):
    """
    åŠ è½½ TREC qrel æ ¼å¼æ ‡ç­¾æ–‡ä»¶
    
    æ ¼å¼: query_id  iteration  doc_id  relevance
    ç¤ºä¾‹: B.301     0          60069   3
    
    Returns:
        Dict[query_id, Dict[doc_id, relevance_score]]
    """
    labels = {}
    
    logger.info(f"ğŸ“‚ Loading qrel labels from {qrel_path}")
    
    with open(qrel_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            parts = line.strip().split('\t')
            
            # éªŒè¯æ ¼å¼
            if len(parts) < 4:
                logger.warning(f"Line {line_num}: Invalid format (expected 4 fields, got {len(parts)})")
                continue
            
            qid, iteration, doc_id, rel = parts[:4]
            
            # åˆå§‹åŒ–æŸ¥è¯¢å­—å…¸
            if qid not in labels:
                labels[qid] = {}
            
            # å­˜å‚¨ç›¸å…³æ€§åˆ†æ•°
            try:
                labels[qid][doc_id] = int(rel)
            except ValueError:
                logger.warning(f"Line {line_num}: Invalid relevance score '{rel}', skipping")
                continue
    
    logger.info(f"âœ… Loaded {len(labels)} queries with relevance labels")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_rels = sum(len(docs) for docs in labels.values())
    logger.info(f"   Total relevance judgments: {total_rels}")
    
    return labels


def calculate_metrics(all_results, labels):
    """
    è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼šRecall@K, MAP, nDCG@K
    
    Args:
        all_results: Dict[query_id, List[candidate_dict]]
        labels: Dict[query_id, Dict[doc_id, relevance_score]]
    
    Returns:
        Dict with averaged metrics
    """
    maps = []
    ndcgs = []
    recalls = []
    
    # ç»Ÿè®¡æœªæ‰¾åˆ°æ ‡ç­¾çš„æŸ¥è¯¢
    missing_labels = 0
    
    for qid, candidates in all_results.items():
        if qid not in labels:
            logger.debug(f"Query {qid} has no ground truth labels, skipping")
            missing_labels += 1
            continue
        
        gt_dict = labels[qid]  # Dict[doc_id, relevance_score]
        
        # æå–æ£€ç´¢åˆ°çš„ ID åˆ—è¡¨
        pred_ids = [c['formula_id'] for c in candidates]
        
        # --- 1. Recall@K (äºŒå€¼åŒ–ç‰ˆæœ¬) ---
        hits = sum(1 for fid in pred_ids if fid in gt_dict)
        recalls.append(1 if hits > 0 else 0)
        
        # --- 2. Average Precision (AP) ---
        ap = 0.0
        relevant_found = 0
        
        for i, fid in enumerate(pred_ids):
            if fid in gt_dict and gt_dict[fid] > 0:  # è€ƒè™‘ç›¸å…³æ€§åˆ†æ•°
                relevant_found += 1
                ap += relevant_found / (i + 1)
        
        total_relevant = sum(1 for score in gt_dict.values() if score > 0)
        maps.append(ap / max(1, total_relevant))
        
        # --- 3. nDCG@K ---
        # è®¡ç®— DCG (Discounted Cumulative Gain)
        dcg = 0.0
        for i, fid in enumerate(pred_ids):
            rel = gt_dict.get(fid, 0)
            dcg += (2**rel - 1) / np.log2(i + 2)
        
        # âœ… ä¿®æ­£ IDCG è®¡ç®—ï¼šä½¿ç”¨å›ºå®šçš„ K
        k = len(pred_ids)
        ideal_rels = sorted(gt_dict.values(), reverse=True)[:k]  # â† ä¿®æ­£ï¼šå– Top-K ä¸ªæœ€ç›¸å…³çš„
        
        idcg = 0.0
        for i, rel in enumerate(ideal_rels):
            idcg += (2**rel - 1) / np.log2(i + 2)
        
        ndcgs.append(dcg / idcg if idcg > 0 else 0)
    
    # æ—¥å¿—è¾“å‡º
    if missing_labels > 0:
        logger.warning(f"âš ï¸  {missing_labels} queries have no ground truth labels")
    
    logger.info(f"ğŸ“Š Evaluated {len(recalls)} queries")
    
    return {
        "Recall@K": float(np.mean(recalls)) if recalls else 0.0,
        "MAP": float(np.mean(maps)) if maps else 0.0,
        "nDCG@K": float(np.mean(ndcgs)) if ndcgs else 0.0,
        "num_evaluated_queries": len(recalls)
    }


def evaluate(pipeline, queries, labels):
    """
    è¿è¡Œè¯„ä¼°å¾ªç¯å¹¶è¿”å›æŒ‡æ ‡å’Œå…¨é‡ç»“æœ
    
    Args:
        pipeline: SearchPipeline instance
        queries: List[dict] with query_id and latex
        labels: Dict[query_id, Dict[doc_id, relevance]]
    
    Returns:
        (metrics: dict, all_results: dict)
    """
    all_results = {}
    
    # è¿›åº¦æ¡åˆå§‹åŒ–
    progress_bar = tqdm(queries, desc="ğŸ” Evaluating", unit="query", leave=True)
    
    failed_count = 0
    
    for query in progress_bar:
        qid = query["query_id"]
        progress_bar.set_postfix({"current_id": qid, "failed": failed_count})
        
        # æ‰§è¡Œæ£€ç´¢
        try:
            results = pipeline.search(query)
            all_results[qid] = results
        except Exception as e:
            logger.error(f"âŒ Error processing query {qid}: {e}")
            all_results[qid] = []
            failed_count += 1
    
    # ç»Ÿè®¡å¤±è´¥ç‡
    if failed_count > 0:
        logger.warning(f"âš ï¸  {failed_count}/{len(queries)} queries failed")
    
    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    metrics = calculate_metrics(all_results, labels)
    
    return metrics, all_results


def save_trec_run(results, output_path, run_id="math-retrieval-system"):
    """
    ä¿å­˜ä¸ºå®˜æ–¹ TREC è¯„æµ‹æ ¼å¼
    
    æ ¼å¼: query_id Q0 doc_id rank score run_id
    """
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            for qid, candidates in results.items():
                for rank, cand in enumerate(candidates, 1):
                    # å°è¯•å¤šä¸ªåˆ†æ•°å­—æ®µ
                    score = cand.get('final_score', cand.get('rank_score', 0.0))
                    f.write(f"{qid} Q0 {cand['formula_id']} {rank} {score:.6f} {run_id}\n")
        
        logger.info(f"ğŸ’¾ TREC run saved to {output_path}")
    except Exception as e:
        logger.error(f"âŒ Failed to save TREC run: {e}")



