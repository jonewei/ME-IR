import json
import sqlite3
import torch
import faiss
import numpy as np
import re
from sentence_transformers import SentenceTransformer
from retrieval.approach0_hash import DualHashGenerator
from pathlib import Path

# ==================== é…ç½® ====================
DB_PATH = "artifacts/formula_index.db"
VECTOR_INDEX_PATH = "artifacts/vector_index_full_v3.faiss"
MAPPING_PATH = "artifacts/vector_id_mapping_v3.json"
FORMULA_JSON = "data/processed/formulas.json"
LABEL_PATH = "data/processed/relevance_labels.json"
QUERY_PATH = "data/processed/queries_full.json"
MODEL_NAME = 'math-similarity/Bert-MLM_arXiv-MP-class_zbMath'

def clean_latex(latex):
    if not latex: return ""
    latex = re.sub(r'\$\$?|\\\[|\\\]', '', latex)
    latex = re.sub(r'\s+', ' ', latex)
    return latex.strip()

class DualPathAnalyzer:
    def __init__(self):
        print("ğŸ“¦ æ­£åœ¨åŠ è½½èµ„æº (æ­¤æ­¥éœ€æ¶ˆè€—å¤§é‡å†…å­˜)...")
        self.conn = sqlite3.connect(DB_PATH)
        self.cursor = self.conn.cursor()
        self.hash_gen = DualHashGenerator()
        
        self.model = SentenceTransformer(MODEL_NAME, device="cuda")
        self.index = faiss.read_index(VECTOR_INDEX_PATH)
        with open(MAPPING_PATH, 'r') as f:
            self.fids = json.load(f)
        
        with open(LABEL_PATH, 'r') as f:
            self.relevance = json.load(f)
        with open(QUERY_PATH, 'r') as f:
            self.queries = json.load(f)
        with open(FORMULA_JSON, 'r') as f:
            self.corpus = json.load(f)

    def eval_stage1_hash(self):
        """è¯„æµ‹ Stage 1: ç»“æ„åŒ–å“ˆå¸Œå¬å›ç‡"""
        print("\n--- [Stage 1: å“ˆå¸Œå¬å›è¯„æµ‹] ---")
        recall_list = []
        for topic_id, query_latex in self.queries.items():
            gt_ids = set(self.relevance.get(topic_id, {}).keys())
            if not gt_ids: continue
            
            # ç”ŸæˆæŸ¥è¯¢ DNA
            dna = self.hash_gen.generate(query_latex)
            
            # ä»æ•°æ®åº“ä¸­å¯»æ‰¾ DNA å®Œå…¨ä¸€è‡´çš„å…¬å¼
            self.cursor.execute("SELECT formula_id FROM formulas WHERE dna = ?", (dna,))
            retrieved_ids = {str(row[0]) for row in self.cursor.fetchall()}
            
            hits = len(gt_ids.intersection(retrieved_ids))
            recall = hits / len(gt_ids)
            recall_list.append(recall)
            
        print(f"âœ… å“ˆå¸Œå¹³å‡å¬å›ç‡ (Recall): {np.mean(recall_list)*100:.2f}%")

    def analyze_failure(self):
        """åˆ†æå¤±è´¥æ¡ˆä¾‹ï¼šä¸ºä»€ä¹ˆåœ¨åº“é‡Œå´æœä¸åˆ°ï¼Ÿ"""
        print("\n--- [å‘é‡æ£€ç´¢å¤±è´¥æ·±åº¦åˆ†æ] ---")
        
        # å¯»æ‰¾ä¸€ä¸ªæ ‡å‡†ç­”æ¡ˆåœ¨åº“ä¸­ï¼Œä½†å‘é‡ Top-1000 æ²¡æœåˆ°çš„ä¾‹å­
        for topic_id, query_latex in self.queries.items():
            gt_dict = self.relevance.get(topic_id, {})
            if not gt_dict: continue
            
            # 1. ç¼–ç æŸ¥è¯¢å‘é‡
            q_emb = self.model.encode([clean_latex(query_latex)], normalize_embeddings=True)[0]
            
            # 2. æ‰§è¡Œ Top-1000 æ£€ç´¢
            distances, indices = self.index.search(np.array([q_emb]).astype('float32'), 1000)
            retrieved_fids = {str(self.fids[idx]) for idx in indices[0] if idx != -1}
            
            # 3. å¯»æ‰¾ä¸€ä¸ªâ€œé—ç â€ï¼šåœ¨åº“é‡Œï¼ˆcorpusï¼‰ä½†ä¸åœ¨æ£€ç´¢ç»“æœé‡Œï¼ˆretrieved_fidsï¼‰çš„æ ‡å‡†ç­”æ¡ˆ
            missed_gt_id = None
            for gt_id in gt_dict.keys():
                if str(gt_id) in self.corpus and str(gt_id) not in retrieved_fids:
                    missed_gt_id = str(gt_id)
                    break
            
            if missed_gt_id:
                print(f"ğŸ” å‘ç°å…¸å‹å¤±è´¥æ¡ˆä¾‹ (Topic: {topic_id}):")
                print(f"   Query LaTeX: {query_latex}")
                
                # è·å–è¯¥é—ç çš„ LaTeX å¹¶è®¡ç®—å‘é‡è·ç¦»
                gt_latex = self.corpus[missed_gt_id]['latex_norm']
                gt_emb = self.model.encode([clean_latex(gt_latex)], normalize_embeddings=True)[0]
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                # å› ä¸ºå‘é‡å·²å½’ä¸€åŒ–ï¼Œç‚¹ç§¯å³ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = np.dot(q_emb, gt_emb)
                
                print(f"   Missed GT ID: {missed_gt_id}")
                print(f"   Missed GT LaTeX: {gt_latex}")
                print(f"   ğŸ“‰ è¯­ä¹‰ç›¸ä¼¼åº¦å¾—åˆ†: {similarity:.4f}")
                print(f"   (æ³¨ï¼š1.0 ä¸ºå®Œç¾åŒ¹é…ï¼Œå½“å‰å¾—åˆ†è¿‡ä½å¯¼è‡´è·Œå‡º Top-1000)")
                
                # é¢å¤–æ£€æŸ¥ï¼šè¿™ä¸¤ä¸ªå…¬å¼çš„ DNA æ˜¯å¦ä¸€è‡´ï¼Ÿ
                q_dna = self.hash_gen.generate(query_latex)
                gt_dna = self.hash_gen.generate(gt_latex)
                print(f"   DNA åŒ¹é…çŠ¶æ€: {'âœ… ä¸€è‡´' if q_dna == gt_dna else 'âŒ ä¸ä¸€è‡´'}")
                break

if __name__ == "__main__":
    analyzer = DualPathAnalyzer()
    analyzer.eval_stage1_hash()
    analyzer.analyze_failure()