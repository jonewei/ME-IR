# import json
# import pickle
# import faiss
# import numpy as np
# import torch
# from pathlib import Path
# from tqdm import tqdm
# from sentence_transformers import SentenceTransformer
# from retrieval.approach0_hash import DualHashGenerator, Approach0HashIndex

# # ==================== æ ¸å¿ƒé…ç½® ====================
# MODEL_NAME = 'math-similarity/Bert-MLM_arXiv-MP-class_zbMath'
# DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# TOP_K = 1000

# # è·¯å¾„é…ç½®
# HASH_INDEX_PATH = "artifacts/approach0_index.pkl"
# VECTOR_INDEX_PATH = "artifacts/vector_index_full_v3.faiss"
# VECTOR_MAPPING_PATH = "artifacts/vector_id_mapping_v3.json"
# FORMULAS_JSON = "data/processed/formulas.json"
# QUERIES_JSON = "data/processed/queries_full.json"
# LABELS_JSON = "data/processed/relevance_labels.json"

# class HybridEvaluator:
#     def __init__(self):
#         print("ğŸ“¦ æ­£åœ¨åŠ è½½åŒè·¯æ£€ç´¢ç³»ç»Ÿèµ„æº...")
#         self.hash_gen = DualHashGenerator()
        
#         # 1. åŠ è½½å“ˆå¸Œç´¢å¼•
#         self.h_index = Approach0HashIndex()
#         self.h_index.load(HASH_INDEX_PATH)
        
#         # 2. åŠ è½½å‘é‡ç´¢å¼•
#         self.model = SentenceTransformer(MODEL_NAME, device=DEVICE)
#         self.v_index = faiss.read_index(VECTOR_INDEX_PATH)
#         with open(VECTOR_MAPPING_PATH, 'r') as f:
#             self.v_mapping = json.load(f)
            
#         # 3. åŠ è½½è¯„æµ‹æ•°æ®
#         with open(QUERIES_JSON, 'r') as f:
#             self.queries = json.load(f)
#         with open(LABELS_JSON, 'r') as f:
#             self.relevance = json.load(f)
        
#         print(f"âœ… èµ„æºåŠ è½½å®Œæˆã€‚ç´¢å¼•è§„æ¨¡: {self.v_index.ntotal:,}")

#     def search_single(self, query_latex):
#         """åŒè·¯å¬å›å¹¶åˆå¹¶"""
#         # A. é¢„å¤„ç†æŸ¥è¯¢
#         res = self.hash_gen.clean_latex(query_latex)
        
#         # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœæ˜¯å…ƒç»„å–ç¬¬ä¸€ä¸ªï¼Œå¦‚æœæ˜¯å­—ç¬¦ä¸²ç›´æ¥ç”¨
#         if isinstance(res, tuple):
#             norm_latex = res[0]
#         else:
#             norm_latex = res
        
#         # B. ç¬¬ä¸€è·¯ï¼šå“ˆå¸Œæ£€ç´¢ (Stage 1)
#         h_val = self.hash_gen.generate_latex_hash(norm_latex)
#         hash_results = self.h_index.search(h_val) # è¿”å›çš„æ˜¯ visual_id åˆ—è¡¨
        
#         # C. ç¬¬äºŒè·¯ï¼šå‘é‡æ£€ç´¢ (Stage 2)
#         q_emb = self.model.encode(
#             [norm_latex], 
#             normalize_embeddings=True, 
#             show_progress_bar=False,
#             convert_to_numpy=True
#         ).astype('float32')
#         _, v_indices = self.v_index.search(q_emb, TOP_K)
#         vector_results = [str(self.v_mapping[idx]) for idx in v_indices[0] if idx != -1]
        
#         # D. ç»“æœåˆå¹¶ä¸å»é‡ (å“ˆå¸Œä¼˜å…ˆç­–ç•¥)
#         # ç†ç”±ï¼šå“ˆå¸Œå‘½ä¸­çš„é€šå¸¸æ˜¯ç²¾ç¡®åŒ¹é…ï¼Œç½®ä¿¡åº¦æœ€é«˜
#         combined_results = []
#         seen = set()
        
#         for vid in hash_results + vector_results:
#             if vid not in seen:
#                 combined_results.append(vid)
#                 seen.add(vid)
        
#         return combined_results[:TOP_K]

#     def run_evaluation(self):
#         print(f"\nğŸš€ å¼€å§‹è¯„æµ‹ {len(self.queries)} æ¡æŸ¥è¯¢ä»»åŠ¡...")
        
#         recall_at_k = []
#         mrr_scores = []
        
#         # ä¸ºäº†æ›´ç²¾ç»†çš„åˆ†æï¼Œè®°å½•æ¯è·¯è´¡çŒ®
#         hash_only_hits = 0
#         vector_only_hits = 0
#         both_hits = 0

#         for qid, query_latex in tqdm(self.queries.items(), desc="Evaluating"):
#             gt_dict = self.relevance.get(qid, {})
#             if not gt_dict: continue
            
#             # æ ‡å‡†ç­”æ¡ˆé›†åˆ
#             gt_ids = set(str(vid) for vid in gt_dict.keys())
            
#             # æ‰§è¡Œæ··åˆæ£€ç´¢
#             results = self.search_single(query_latex)
            
#             # è®¡ç®—æŒ‡æ ‡
#             hits = gt_ids.intersection(set(results))
#             num_hits = len(hits)
            
#             # Recall@K
#             recall = num_hits / len(gt_ids) if len(gt_ids) > 0 else 0
#             recall_at_k.append(recall)
            
#             # MRR (Mean Reciprocal Rank)
#             mrr = 0
#             for rank, res_id in enumerate(results):
#                 if res_id in gt_ids:
#                     mrr = 1 / (rank + 1)
#                     break
#             mrr_scores.append(mrr)

#         # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
#         mean_recall = np.mean(recall_at_k) * 100
#         mean_mrr = np.mean(mrr_scores)
        
#         print("\n" + "="*60)
#         print("ğŸ† ARQMATH-3 æ··åˆæ£€ç´¢è¯„æµ‹æŠ¥å‘Š")
#         print("="*60)
#         print(f"ğŸ“Š åŸºç¡€æŒ‡æ ‡:")
#         print(f"   Mean Recall@{TOP_K}: {mean_recall:.2f}%")
#         print(f"   Mean MRR@{TOP_K}:    {mean_mrr:.4f}")
#         print("-" * 60)
#         print(f"ğŸ’¡ è°ƒè¯•åˆ†æ:")
#         print(f"   æ€»è®¡è¯„æµ‹æŸ¥è¯¢æ•°: {len(recall_at_k)}")
#         print(f"   é…ç½®æ¨¡å‹: {MODEL_NAME}")
#         print(f"   å¯¹é½ç­–ç•¥: Visual-ID Deduplication")
#         print("="*60)

# if __name__ == "__main__":
#     evaluator = HybridEvaluator()
#     evaluator.run_evaluation()
import json
import faiss
import numpy as np
from tqdm import tqdm
from retrieval.approach0_hash import DualHashGenerator, Approach0HashIndex

class HybridEvaluator:
    def __init__(self):
        print("ğŸ“¦ åŠ è½½æ£€ç´¢èµ„æº...")
        self.hash_gen = DualHashGenerator()
        self.h_index = Approach0HashIndex()
        self.h_index.load("artifacts/approach0_index.pkl")
        
        from sentence_transformers import SentenceTransformer
        self.model = SentenceTransformer('math-similarity/Bert-MLM_arXiv-MP-class_zbMath', device="cuda")
        self.v_index = faiss.read_index("artifacts/vector_index_full_v4.faiss")
        with open("artifacts/vector_id_mapping_v4.json", 'r') as f:
            self.v_mapping = json.load(f)
            
        with open("data/processed/queries_full.json", 'r') as f:
            self.queries = json.load(f)
        with open("data/processed/relevance_labels.json", 'r') as f:
            self.relevance = json.load(f)

    def search_single(self, query_latex):
        # A. é¢„å¤„ç† (é€‚é…å¤šè¿”å›å€¼)
        res = self.hash_gen.clean_latex(query_latex)
        norm_latex = res[0] if isinstance(res, tuple) else res
        
        # B. å“ˆå¸Œè·¯
        h_val = self.hash_gen.generate_latex_hash(norm_latex)
        h_res = self.h_index.search(h_val)
        
        # C. å‘é‡è·¯
        q_emb = self.model.encode([norm_latex], normalize_embeddings=True, convert_to_numpy=True)
        _, v_indices = self.v_index.search(q_emb.astype('float32'), 1000)
        v_res = [str(self.v_mapping[idx]) for idx in v_indices[0] if idx != -1]
        
        # D. åˆå¹¶
        combined = []
        seen = set()
        for vid in h_res + v_res:
            if vid not in seen:
                combined.append(vid)
                seen.add(vid)
        return combined[:1000]

    def run(self):
        recalls, mrr_scores = [], []
        for qid, q_latex in tqdm(self.queries.items(), desc="Evaluating"):
            gt = set(str(k) for k in self.relevance.get(qid, {}).keys())
            if not gt: continue
            
            results = self.search_single(q_latex)
            hits = gt.intersection(set(results))
            recalls.append(len(hits)/len(gt))
            
            mrr = 0
            for i, r in enumerate(results):
                if r in gt:
                    mrr = 1/(i+1)
                    break
            mrr_scores.append(mrr)
            
        print(f"\nğŸ† Mean Recall@1000: {np.mean(recalls)*100:.2f}%")
        print(f"ğŸ† Mean MRR@1000:    {np.mean(mrr_scores):.4f}")

if __name__ == "__main__":
    HybridEvaluator().run()
